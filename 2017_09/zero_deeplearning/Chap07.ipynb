{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./train.csv\")[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# datasetを読み込む\n",
    "t_temp = dataset[[0]].values.ravel().astype(np.uint8)\n",
    "x_dataset = np.array(dataset.iloc[:,1:].values.astype(np.uint8))/256.0\n",
    "n_dataset = len(t_temp)\n",
    "t_dataset = np.zeros((n_dataset,10),np.uint8)\n",
    "t_dataset[np.arange(n_dataset),t_temp]=1 # 1-hot 表現にする\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ランダムシャッフルする\n",
    "index = np.arange(n_dataset)\n",
    "np.random.shuffle(index)\n",
    "\n",
    "x_dataset = x_dataset[index].reshape(len(x_dataset),1,28,28)\n",
    "t_dataset = t_dataset[index]\n",
    "\n",
    "# 教師データとテストデータに分割\n",
    "n_test = n_dataset / 4\n",
    "n_train = n_dataset - n_test \n",
    "x_train , t_train, x_test, t_test = \\\n",
    "    x_dataset[:n_train],t_dataset[:n_train],x_dataset[n_train:],t_dataset[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def im2col(input_data,filter_h,filter_w,stride=1,pad=0):\n",
    "    N,C,H,W = input_data.shape\n",
    "    out_h = (H+2*pad-filter_h)/stride+1\n",
    "    out_w = (W+2*pad-filter_w)/stride+1\n",
    "    \n",
    "    img = np.pad(input_data,[(0,0),(0,0),(pad,pad),(pad,pad)],\"constant\")\n",
    "    col = np.zeros((N,C,filter_h,filter_w,out_h,out_w))\n",
    "    \n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride * out_w\n",
    "            col[:,:,y,x,:,:] = img[:,:,y:y_max:stride,x:x_max:stride]\n",
    "    \n",
    "    col = col.transpose(0,4,5,1,2,3).reshape(N*out_h*out_w,-1)\n",
    "    return col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def col2im(col,input_shape,filter_h,filter_w,stride=1,pad=0):\n",
    "    N,C,H,W = input_shape\n",
    "    \n",
    "    out_h =(H+2*pad-filter_h)/stride + 1\n",
    "    out_w = (W+2*pad-filter_w)/stride + 1\n",
    "    col = col.reshape(N,out_h,out_w,C,filter_h,filter_w).transpose(0,3,4,5,1,2)\n",
    "    \n",
    "    img = np.zeros((N,C,H+2*pad+stride-1,W+2*pad+stride-1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:,:,y:y_max:stride,x:x_max:stride] += col[:,:,y,x,:,:]\n",
    "    \n",
    "    return img[:,:,pad:H+pad,pad:W+pad]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n"
     ]
    }
   ],
   "source": [
    "x1 = np.random.rand(1,3,7,7)\n",
    "col1 = im2col(x1,5,5,stride=1,pad=0)\n",
    "print(col1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "x1 = np.random.rand(10,3,7,7)\n",
    "col1 = im2col(x1,5,5,stride=1,pad=0)\n",
    "print(col1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self,W,b,stride=1,pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self,x):\n",
    "        FN,C,FH,FW = self.W.shape\n",
    "        N,C,H,W = x.shape\n",
    "        \n",
    "        out_h = int(1+(H+2*self.pad - FH)/self.stride)\n",
    "        out_w = int(1+(W+2*self.pad - FW)/self.stride)\n",
    "        \n",
    "        col = im2col(x,FH,FW,self.stride,self.pad)\n",
    "        col_W = self.W.reshape(FN,-1).T\n",
    "        \n",
    "        out = np.dot(col,col_W)+self.b\n",
    "        out = out.reshape(N,out_h,out_w,-1).transpose(0,3,1,2)\n",
    "        \n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        FN,C,FH,FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1,FN)\n",
    "        \n",
    "        self.db = np.sum(dout,axis=0)\n",
    "        self.dW = np.dot(self.col.T,dout)\n",
    "        self.dW = self.dW.transpose(1,0).reshape(FN,C,FH,FW)\n",
    "        \n",
    "        dcol = np.dot(dout,self.col_W.T)\n",
    "        dx = col2im(dcol,self.x.shape,FH,FW,self.stride,self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self,pool_h,pool_w,stride=1,pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "    \n",
    "    def forward(self,x):\n",
    "        N,C,H,W = x.shape\n",
    "        out_h = int(1+(H-self.pool_h)/self.stride)\n",
    "        out_w = int(1+(W-self.pool_w)/self.stride)\n",
    "        \n",
    "        col = im2col(x,self.pool_h,self.pool_w,self.stride,self.pad)\n",
    "        col = col.reshape(-1,self.pool_h*self.pool_w)\n",
    "         \n",
    "        arg_max = np.argmax(col,axis=1)\n",
    "        out = np.max(col,axis=1)\n",
    "        out = out.reshape(N,out_h,out_w,C).transpose(0,3,1,2)\n",
    "        \n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dout = dout.transpose(0,2,3,1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size,pool_size))\n",
    "        dmax[np.arange(self.arg_max.size),self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,))\n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0]*dmax.shape[1] * dmax.shape[2],-1)\n",
    "        dx = col2im(dcol,self.x.shape,self.pool_h,self.pool_w,self.stride,self.pad)\n",
    "        \n",
    "        return dx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self,x,y):\n",
    "        out = x+y\n",
    "        return out\n",
    "\n",
    "    def backward(self,dout):\n",
    "        return dout,dout #dx,dy\n",
    "    \n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self,x):\n",
    "        self.mask = (x<=0)\n",
    "        out = x.copy()\n",
    "        out[self.mask]=0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = dout.copy()\n",
    "        dx[self.mask] = 0\n",
    "        return dx\n",
    "    \n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = 1.0/(1.0+np.exp(-x))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def forward(self,dout):\n",
    "        dx = dout * self.out * ( 1.0- self.out )\n",
    "        return dx\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self,W,b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        \n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self,x):\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        self.x = x\n",
    "        \n",
    "        out = np.dot(self.x,self.W)+self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = np.dot(dout,self.W.T)\n",
    "        self.dW = np.dot(self.x.T,dout)\n",
    "        self.db = np.sum(dout,axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)\n",
    "        return dx\n",
    "    \n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "    \n",
    "    def forward(self,x,t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y,self.t)\n",
    "        \n",
    "        return self.loss # スカラー\n",
    "    \n",
    "    def backward(self,dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t)/batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    def __init__(self,input_dim=(1,28,28),\n",
    "                conv_param={\"filter_num\":30,\n",
    "                            \"filter_size\":5,\n",
    "                            \"pad\":0,\n",
    "                            \"stride\":1},\n",
    "                hidden_size=100,output_size=10,weight_init_std=0.01):\n",
    "        filter_num = conv_param[\"filter_num\"]\n",
    "        filter_size = conv_param[\"filter_size\"]\n",
    "        filter_pad = conv_param[\"pad\"]\n",
    "        filter_stride = conv_param[\"stride\"]\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2 * filter_pad)/filter_stride + 1\n",
    "        pool_output_size = int( filter_num * (conv_output_size/2)*(conv_output_size/2))\n",
    "        \n",
    "        self.params={}\n",
    "        self.params[\"W1\"] = weight_init_std * np.random.randn(filter_num,input_dim[0],\n",
    "                                                             filter_size,filter_size)\n",
    "        self.params[\"b1\"] = np.zeros(filter_num)\n",
    "        self.params[\"W2\"] = weight_init_std * np.random.randn(pool_output_size,hidden_size)\n",
    "        self.params[\"b2\"] = np.zeros(hidden_size)\n",
    "        self.params[\"W3\"] = weight_init_std * np.random.randn(hidden_size,output_size)\n",
    "        self.params[\"b3\"] = np.zeros(output_size)\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers[\"Conv1\"] = Convolution(self.params[\"W1\"],\n",
    "                                          self.params[\"b1\"],\n",
    "                                          conv_param[\"stride\"],\n",
    "                                          conv_param[\"pad\"])\n",
    "        self.layers[\"Relu1\"] = Relu()\n",
    "        self.layers[\"Pool1\"] = Pooling(pool_h=2,pool_w=2,stride=2)\n",
    "        self.layers[\"Affine1\"] = Affine(self.params[\"W2\"],self.params[\"b2\"])\n",
    "        self.layers[\"Relu2\"] = Relu()\n",
    "        self.layers[\"Affine2\"] = Affine(self.params[\"W3\"],self.params[\"b3\"])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "    \n",
    "    def predict(self,x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y,t)\n",
    "    \n",
    "    def accuracy(self,x,t,batch_size=100):\n",
    "        if t.ndim != 1: t = np.argmax(t,axis=1)\n",
    "            \n",
    "        acc=0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0]/batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y,axis=1)\n",
    "            acc += np.sum(y==tt)\n",
    "        return acc / x.shape[0]\n",
    "        \n",
    "    def gradient(self,x,t):\n",
    "        self.loss(x,t)\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        grads = {}\n",
    "        grads[\"W1\"] = self.layers[\"Conv1\"].dW\n",
    "        grads[\"b1\"] = self.layers[\"Conv1\"].db\n",
    "        grads[\"W2\"] = self.layers[\"Affine1\"].dW\n",
    "        grads[\"b2\"] = self.layers[\"Affine1\"].db\n",
    "        grads[\"W3\"] = self.layers[\"Affine2\"].dW\n",
    "        grads[\"b3\"] = self.layers[\"Affine2\"].db\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    \n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "    \n",
    "    def forward(self,x,t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y,self.t)\n",
    "        \n",
    "        return self.loss # スカラー\n",
    "    \n",
    "    def backward(self,dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t)/batch_size\n",
    "        \n",
    "        return dx\n",
    "\n",
    "def cross_entropy_error(y,t,eps=1e-8):\n",
    "    \"\"\" common/functions.py  \"\"\"\n",
    "    \n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1,t.size)\n",
    "        y = y.reshape(1,y.size)\n",
    "    \n",
    "    # one-hot-vectorから正解ラベルのインデックスに\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "    batch_size = y.shape[0]\n",
    "    # print y\n",
    "    # print y[np.arange(batch_size),t]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size),t]+eps))/batch_size\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\" common/functions.py  \"\"\"\n",
    "    \n",
    "    \n",
    "    # mnistとかやる場合は2次元配列(データ数x入力次元)\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x,axis=0)\n",
    "        exp_x = np.exp(x)\n",
    "        y = exp_x / np.sum(exp_x,axis=0)\n",
    "        return y.T\n",
    "    \n",
    "    x = x - np.max(x)\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\" シグモイド関数 \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3150: train loss: 2.301510\n",
      "1/3150: train loss: 2.301446\n",
      "2/3150: train loss: 2.300941\n",
      "3/3150: train loss: 2.300041\n",
      "4/3150: train loss: 2.301475\n",
      "5/3150: train loss: 2.301925\n",
      "6/3150: train loss: 2.298563\n",
      "7/3150: train loss: 2.297033\n",
      "8/3150: train loss: 2.299214\n",
      "9/3150: train loss: 2.297654\n",
      "10/3150: train loss: 2.299246\n",
      "11/3150: train loss: 2.297932\n",
      "12/3150: train loss: 2.301088\n",
      "13/3150: train loss: 2.298624\n",
      "14/3150: train loss: 2.298901\n",
      "15/3150: train loss: 2.299654\n",
      "16/3150: train loss: 2.299304\n",
      "17/3150: train loss: 2.299327\n",
      "18/3150: train loss: 2.295026\n",
      "19/3150: train loss: 2.296343\n",
      "20/3150: train loss: 2.294428\n",
      "21/3150: train loss: 2.295468\n",
      "22/3150: train loss: 2.300519\n",
      "23/3150: train loss: 2.290976\n",
      "24/3150: train loss: 2.290174\n",
      "25/3150: train loss: 2.295719\n",
      "26/3150: train loss: 2.288595\n",
      "27/3150: train loss: 2.290534\n",
      "28/3150: train loss: 2.292359\n",
      "29/3150: train loss: 2.289114\n",
      "30/3150: train loss: 2.287112\n",
      "31/3150: train loss: 2.277898\n",
      "32/3150: train loss: 2.281560\n",
      "33/3150: train loss: 2.265488\n",
      "34/3150: train loss: 2.278206\n",
      "35/3150: train loss: 2.261857\n",
      "36/3150: train loss: 2.227549\n",
      "37/3150: train loss: 2.237575\n",
      "38/3150: train loss: 2.222570\n",
      "39/3150: train loss: 2.190294\n",
      "40/3150: train loss: 2.130841\n",
      "41/3150: train loss: 2.112716\n",
      "42/3150: train loss: 2.021789\n",
      "43/3150: train loss: 2.040356\n",
      "44/3150: train loss: 1.881922\n",
      "45/3150: train loss: 1.824112\n",
      "46/3150: train loss: 1.818115\n",
      "47/3150: train loss: 1.684386\n",
      "48/3150: train loss: 1.622447\n",
      "49/3150: train loss: 1.442273\n",
      "50/3150: train loss: 1.616463\n",
      "51/3150: train loss: 1.679817\n",
      "52/3150: train loss: 1.629365\n",
      "53/3150: train loss: 1.596331\n",
      "54/3150: train loss: 1.372564\n",
      "55/3150: train loss: 1.275687\n",
      "56/3150: train loss: 1.232502\n",
      "57/3150: train loss: 1.080453\n",
      "58/3150: train loss: 1.158368\n",
      "59/3150: train loss: 1.102829\n",
      "60/3150: train loss: 1.127872\n",
      "61/3150: train loss: 1.036036\n",
      "62/3150: train loss: 0.933384\n",
      "63/3150: train loss: 0.947644\n",
      "64/3150: train loss: 0.851059\n",
      "65/3150: train loss: 0.751375\n",
      "66/3150: train loss: 0.725266\n",
      "67/3150: train loss: 0.767736\n",
      "68/3150: train loss: 0.641400\n",
      "69/3150: train loss: 0.704026\n",
      "70/3150: train loss: 0.744870\n",
      "71/3150: train loss: 0.873435\n",
      "72/3150: train loss: 0.842719\n",
      "73/3150: train loss: 0.867016\n",
      "74/3150: train loss: 0.663527\n",
      "75/3150: train loss: 0.597733\n",
      "76/3150: train loss: 0.729334\n",
      "77/3150: train loss: 0.692608\n",
      "78/3150: train loss: 0.568885\n",
      "79/3150: train loss: 0.583267\n",
      "80/3150: train loss: 0.761811\n",
      "81/3150: train loss: 0.688454\n",
      "82/3150: train loss: 0.507326\n",
      "83/3150: train loss: 0.552299\n",
      "84/3150: train loss: 0.629531\n",
      "85/3150: train loss: 0.476819\n",
      "86/3150: train loss: 0.565274\n",
      "87/3150: train loss: 0.462011\n",
      "88/3150: train loss: 0.462620\n",
      "89/3150: train loss: 0.592537\n",
      "90/3150: train loss: 0.588469\n",
      "91/3150: train loss: 0.387958\n",
      "92/3150: train loss: 0.410174\n",
      "93/3150: train loss: 0.437124\n",
      "94/3150: train loss: 0.496123\n",
      "95/3150: train loss: 0.489343\n",
      "96/3150: train loss: 0.581855\n",
      "97/3150: train loss: 0.487081\n",
      "98/3150: train loss: 0.468634\n",
      "99/3150: train loss: 0.528536\n",
      "100/3150: train loss: 0.504136\n",
      "101/3150: train loss: 0.313113\n",
      "102/3150: train loss: 0.588348\n",
      "103/3150: train loss: 0.335216\n",
      "104/3150: train loss: 0.484623\n",
      "105/3150: train loss: 0.432433\n",
      "106/3150: train loss: 0.556769\n",
      "107/3150: train loss: 0.434484\n",
      "108/3150: train loss: 0.535020\n",
      "109/3150: train loss: 0.501144\n",
      "110/3150: train loss: 0.314857\n",
      "111/3150: train loss: 0.446085\n",
      "112/3150: train loss: 0.648188\n",
      "113/3150: train loss: 0.490190\n",
      "114/3150: train loss: 0.419842\n",
      "115/3150: train loss: 0.351157\n",
      "116/3150: train loss: 0.349923\n",
      "117/3150: train loss: 0.490756\n",
      "118/3150: train loss: 0.325269\n",
      "119/3150: train loss: 0.387592\n",
      "120/3150: train loss: 0.612246\n",
      "121/3150: train loss: 0.379607\n",
      "122/3150: train loss: 0.716072\n",
      "123/3150: train loss: 0.418974\n",
      "124/3150: train loss: 0.312132\n",
      "125/3150: train loss: 0.432198\n",
      "126/3150: train loss: 0.451543\n",
      "127/3150: train loss: 0.509145\n",
      "128/3150: train loss: 0.360956\n",
      "129/3150: train loss: 0.355572\n",
      "130/3150: train loss: 0.390021\n",
      "131/3150: train loss: 0.448211\n",
      "132/3150: train loss: 0.462781\n",
      "133/3150: train loss: 0.375636\n",
      "134/3150: train loss: 0.420872\n",
      "135/3150: train loss: 0.341151\n",
      "136/3150: train loss: 0.363281\n",
      "137/3150: train loss: 0.360684\n",
      "138/3150: train loss: 0.382259\n",
      "139/3150: train loss: 0.378759\n",
      "140/3150: train loss: 0.431033\n",
      "141/3150: train loss: 0.379763\n",
      "142/3150: train loss: 0.354968\n",
      "143/3150: train loss: 0.347888\n",
      "144/3150: train loss: 0.342015\n",
      "145/3150: train loss: 0.380992\n",
      "146/3150: train loss: 0.454736\n",
      "147/3150: train loss: 0.270373\n",
      "148/3150: train loss: 0.354443\n",
      "149/3150: train loss: 0.349626\n",
      "150/3150: train loss: 0.341442\n",
      "151/3150: train loss: 0.306720\n",
      "152/3150: train loss: 0.284667\n",
      "153/3150: train loss: 0.383342\n",
      "154/3150: train loss: 0.384326\n",
      "155/3150: train loss: 0.558400\n",
      "156/3150: train loss: 0.441629\n",
      "157/3150: train loss: 0.535005\n",
      "158/3150: train loss: 0.382901\n",
      "159/3150: train loss: 0.288070\n",
      "160/3150: train loss: 0.281506\n",
      "161/3150: train loss: 0.297370\n",
      "162/3150: train loss: 0.327130\n",
      "163/3150: train loss: 0.300832\n",
      "164/3150: train loss: 0.326774\n",
      "165/3150: train loss: 0.310601\n",
      "166/3150: train loss: 0.287360\n",
      "167/3150: train loss: 0.295127\n",
      "168/3150: train loss: 0.293512\n",
      "169/3150: train loss: 0.312328\n",
      "170/3150: train loss: 0.259903\n",
      "171/3150: train loss: 0.233342\n",
      "172/3150: train loss: 0.324191\n",
      "173/3150: train loss: 0.286661\n",
      "174/3150: train loss: 0.293982\n",
      "175/3150: train loss: 0.281299\n",
      "176/3150: train loss: 0.229837\n",
      "177/3150: train loss: 0.307723\n",
      "178/3150: train loss: 0.356988\n",
      "179/3150: train loss: 0.345394\n",
      "180/3150: train loss: 0.201638\n",
      "181/3150: train loss: 0.294780\n",
      "182/3150: train loss: 0.240692\n",
      "183/3150: train loss: 0.390209\n",
      "184/3150: train loss: 0.294803\n",
      "185/3150: train loss: 0.334683\n",
      "186/3150: train loss: 0.314047\n",
      "187/3150: train loss: 0.247964\n",
      "188/3150: train loss: 0.354815\n",
      "189/3150: train loss: 0.280502\n",
      "190/3150: train loss: 0.286499\n",
      "191/3150: train loss: 0.438610\n",
      "192/3150: train loss: 0.306859\n",
      "193/3150: train loss: 0.292487\n",
      "194/3150: train loss: 0.297591\n",
      "195/3150: train loss: 0.304554\n",
      "196/3150: train loss: 0.172895\n",
      "197/3150: train loss: 0.276631\n",
      "198/3150: train loss: 0.384333\n",
      "199/3150: train loss: 0.162420\n",
      "200/3150: train loss: 0.404308\n",
      "201/3150: train loss: 0.246032\n",
      "202/3150: train loss: 0.363265\n",
      "203/3150: train loss: 0.362044\n",
      "204/3150: train loss: 0.285403\n",
      "205/3150: train loss: 0.290583\n",
      "206/3150: train loss: 0.308043\n",
      "207/3150: train loss: 0.258379\n",
      "208/3150: train loss: 0.288038\n",
      "209/3150: train loss: 0.301158\n",
      "210/3150: train loss: 0.244014\n",
      "211/3150: train loss: 0.204169\n",
      "212/3150: train loss: 0.404756\n",
      "213/3150: train loss: 0.417955\n",
      "214/3150: train loss: 0.169040\n",
      "215/3150: train loss: 0.278283\n",
      "216/3150: train loss: 0.436225\n",
      "217/3150: train loss: 0.212176\n",
      "218/3150: train loss: 0.294231\n",
      "219/3150: train loss: 0.257040\n",
      "220/3150: train loss: 0.337902\n",
      "221/3150: train loss: 0.287779\n",
      "222/3150: train loss: 0.262358\n",
      "223/3150: train loss: 0.286084\n",
      "224/3150: train loss: 0.291288\n",
      "225/3150: train loss: 0.296979\n",
      "226/3150: train loss: 0.197913\n",
      "227/3150: train loss: 0.254262\n",
      "228/3150: train loss: 0.224161\n",
      "229/3150: train loss: 0.337765\n",
      "230/3150: train loss: 0.224830\n",
      "231/3150: train loss: 0.302182\n",
      "232/3150: train loss: 0.198381\n",
      "233/3150: train loss: 0.364012\n",
      "234/3150: train loss: 0.320306\n",
      "235/3150: train loss: 0.118862\n",
      "236/3150: train loss: 0.304986\n",
      "237/3150: train loss: 0.306383\n",
      "238/3150: train loss: 0.195463\n",
      "239/3150: train loss: 0.151060\n",
      "240/3150: train loss: 0.153458\n",
      "241/3150: train loss: 0.366470\n",
      "242/3150: train loss: 0.237073\n",
      "243/3150: train loss: 0.214778\n",
      "244/3150: train loss: 0.273991\n",
      "245/3150: train loss: 0.156090\n",
      "246/3150: train loss: 0.330827\n",
      "247/3150: train loss: 0.251564\n",
      "248/3150: train loss: 0.433197\n",
      "249/3150: train loss: 0.236271\n",
      "250/3150: train loss: 0.198828\n",
      "251/3150: train loss: 0.297872\n",
      "252/3150: train loss: 0.198002\n",
      "253/3150: train loss: 0.227806\n",
      "254/3150: train loss: 0.232305\n",
      "255/3150: train loss: 0.285589\n",
      "256/3150: train loss: 0.277614\n",
      "257/3150: train loss: 0.296765\n",
      "258/3150: train loss: 0.173119\n",
      "259/3150: train loss: 0.263396\n",
      "260/3150: train loss: 0.253751\n",
      "261/3150: train loss: 0.229894\n",
      "262/3150: train loss: 0.164250\n",
      "263/3150: train loss: 0.220202\n",
      "264/3150: train loss: 0.371454\n",
      "265/3150: train loss: 0.149344\n",
      "266/3150: train loss: 0.212603\n",
      "267/3150: train loss: 0.224203\n",
      "268/3150: train loss: 0.169188\n",
      "269/3150: train loss: 0.217899\n",
      "270/3150: train loss: 0.292288\n",
      "271/3150: train loss: 0.218730\n",
      "272/3150: train loss: 0.165178\n",
      "273/3150: train loss: 0.194058\n",
      "274/3150: train loss: 0.242809\n",
      "275/3150: train loss: 0.188805\n",
      "276/3150: train loss: 0.142435\n",
      "277/3150: train loss: 0.250724\n",
      "278/3150: train loss: 0.214040\n",
      "279/3150: train loss: 0.284507\n",
      "280/3150: train loss: 0.247683\n",
      "281/3150: train loss: 0.488339\n",
      "282/3150: train loss: 0.139529\n",
      "283/3150: train loss: 0.194020\n",
      "284/3150: train loss: 0.262567\n",
      "285/3150: train loss: 0.229696\n",
      "286/3150: train loss: 0.268862\n",
      "287/3150: train loss: 0.225518\n",
      "288/3150: train loss: 0.189757\n",
      "289/3150: train loss: 0.244631\n",
      "290/3150: train loss: 0.188191\n",
      "291/3150: train loss: 0.266731\n",
      "292/3150: train loss: 0.249337\n",
      "293/3150: train loss: 0.352561\n",
      "294/3150: train loss: 0.217236\n",
      "295/3150: train loss: 0.154529\n",
      "296/3150: train loss: 0.248169\n",
      "297/3150: train loss: 0.283969\n",
      "298/3150: train loss: 0.272636\n",
      "299/3150: train loss: 0.212934\n",
      "300/3150: train loss: 0.135747\n",
      "301/3150: train loss: 0.256093\n",
      "302/3150: train loss: 0.157191\n",
      "303/3150: train loss: 0.248116\n",
      "304/3150: train loss: 0.224326\n",
      "305/3150: train loss: 0.248502\n",
      "306/3150: train loss: 0.167758\n",
      "307/3150: train loss: 0.121868\n",
      "308/3150: train loss: 0.145796\n",
      "309/3150: train loss: 0.175275\n",
      "310/3150: train loss: 0.183575\n",
      "311/3150: train loss: 0.176933\n",
      "312/3150: train loss: 0.305284\n",
      "313/3150: train loss: 0.138082\n",
      "314/3150: train loss: 0.238587\n",
      "(0.9193650793650794, 0.9182857142857143)\n",
      "315/3150: train loss: 0.225481\n",
      "316/3150: train loss: 0.197985\n",
      "317/3150: train loss: 0.203980\n",
      "318/3150: train loss: 0.151096\n",
      "319/3150: train loss: 0.153195\n",
      "320/3150: train loss: 0.232491\n",
      "321/3150: train loss: 0.217610\n",
      "322/3150: train loss: 0.183074\n",
      "323/3150: train loss: 0.120435\n",
      "324/3150: train loss: 0.219009\n",
      "325/3150: train loss: 0.135527\n",
      "326/3150: train loss: 0.148696\n",
      "327/3150: train loss: 0.156646\n",
      "328/3150: train loss: 0.237779\n",
      "329/3150: train loss: 0.180316\n",
      "330/3150: train loss: 0.149635\n",
      "331/3150: train loss: 0.167626\n",
      "332/3150: train loss: 0.196010\n",
      "333/3150: train loss: 0.174088\n",
      "334/3150: train loss: 0.118384\n",
      "335/3150: train loss: 0.192450\n",
      "336/3150: train loss: 0.144724\n",
      "337/3150: train loss: 0.251828\n",
      "338/3150: train loss: 0.301231\n",
      "339/3150: train loss: 0.184090\n",
      "340/3150: train loss: 0.223353\n",
      "341/3150: train loss: 0.162793\n",
      "342/3150: train loss: 0.202137\n",
      "343/3150: train loss: 0.271335\n",
      "344/3150: train loss: 0.228684\n",
      "345/3150: train loss: 0.136730\n",
      "346/3150: train loss: 0.214915\n",
      "347/3150: train loss: 0.301400\n",
      "348/3150: train loss: 0.177729\n",
      "349/3150: train loss: 0.152064\n",
      "350/3150: train loss: 0.280168\n",
      "351/3150: train loss: 0.190361\n",
      "352/3150: train loss: 0.159715\n",
      "353/3150: train loss: 0.212164\n",
      "354/3150: train loss: 0.199934\n",
      "355/3150: train loss: 0.129540\n",
      "356/3150: train loss: 0.127544\n",
      "357/3150: train loss: 0.259635\n",
      "358/3150: train loss: 0.164861\n",
      "359/3150: train loss: 0.138698\n",
      "360/3150: train loss: 0.195425\n",
      "361/3150: train loss: 0.152774\n",
      "362/3150: train loss: 0.295448\n",
      "363/3150: train loss: 0.206815\n",
      "364/3150: train loss: 0.208867\n",
      "365/3150: train loss: 0.236684\n",
      "366/3150: train loss: 0.214040\n",
      "367/3150: train loss: 0.164883\n",
      "368/3150: train loss: 0.326280\n",
      "369/3150: train loss: 0.264089\n",
      "370/3150: train loss: 0.292062\n",
      "371/3150: train loss: 0.207952\n",
      "372/3150: train loss: 0.235989\n",
      "373/3150: train loss: 0.177163\n",
      "374/3150: train loss: 0.203860\n",
      "375/3150: train loss: 0.178667\n",
      "376/3150: train loss: 0.126017\n",
      "377/3150: train loss: 0.164678\n",
      "378/3150: train loss: 0.142047\n",
      "379/3150: train loss: 0.117197\n",
      "380/3150: train loss: 0.127523\n",
      "381/3150: train loss: 0.119739\n",
      "382/3150: train loss: 0.235322\n",
      "383/3150: train loss: 0.136526\n",
      "384/3150: train loss: 0.153014\n",
      "385/3150: train loss: 0.182757\n",
      "386/3150: train loss: 0.137597\n",
      "387/3150: train loss: 0.154895\n",
      "388/3150: train loss: 0.142671\n",
      "389/3150: train loss: 0.306644\n",
      "390/3150: train loss: 0.093772\n",
      "391/3150: train loss: 0.211996\n",
      "392/3150: train loss: 0.289510\n",
      "393/3150: train loss: 0.080727\n",
      "394/3150: train loss: 0.228860\n",
      "395/3150: train loss: 0.163219\n",
      "396/3150: train loss: 0.289292\n",
      "397/3150: train loss: 0.108251\n",
      "398/3150: train loss: 0.121382\n",
      "399/3150: train loss: 0.127643\n",
      "400/3150: train loss: 0.138056\n",
      "401/3150: train loss: 0.207805\n",
      "402/3150: train loss: 0.116606\n",
      "403/3150: train loss: 0.212559\n",
      "404/3150: train loss: 0.198819\n",
      "405/3150: train loss: 0.271941\n",
      "406/3150: train loss: 0.161768\n",
      "407/3150: train loss: 0.170068\n",
      "408/3150: train loss: 0.138707\n",
      "409/3150: train loss: 0.169531\n",
      "410/3150: train loss: 0.100559\n",
      "411/3150: train loss: 0.108595\n",
      "412/3150: train loss: 0.120009\n",
      "413/3150: train loss: 0.144260\n",
      "414/3150: train loss: 0.190249\n",
      "415/3150: train loss: 0.222840\n",
      "416/3150: train loss: 0.090147\n",
      "417/3150: train loss: 0.217493\n",
      "418/3150: train loss: 0.160705\n",
      "419/3150: train loss: 0.236640\n",
      "420/3150: train loss: 0.180031\n",
      "421/3150: train loss: 0.171856\n",
      "422/3150: train loss: 0.196252\n",
      "423/3150: train loss: 0.119215\n",
      "424/3150: train loss: 0.162440\n",
      "425/3150: train loss: 0.127508\n",
      "426/3150: train loss: 0.203033\n",
      "427/3150: train loss: 0.128395\n",
      "428/3150: train loss: 0.118908\n",
      "429/3150: train loss: 0.154325\n",
      "430/3150: train loss: 0.156812\n",
      "431/3150: train loss: 0.199255\n",
      "432/3150: train loss: 0.176938\n",
      "433/3150: train loss: 0.189258\n",
      "434/3150: train loss: 0.168158\n",
      "435/3150: train loss: 0.116901\n",
      "436/3150: train loss: 0.139433\n",
      "437/3150: train loss: 0.099024\n",
      "438/3150: train loss: 0.112819\n",
      "439/3150: train loss: 0.144999\n",
      "440/3150: train loss: 0.165539\n",
      "441/3150: train loss: 0.292631\n",
      "442/3150: train loss: 0.120382\n",
      "443/3150: train loss: 0.157263\n",
      "444/3150: train loss: 0.111668\n",
      "445/3150: train loss: 0.197908\n",
      "446/3150: train loss: 0.132330\n",
      "447/3150: train loss: 0.122893\n",
      "448/3150: train loss: 0.083831\n",
      "449/3150: train loss: 0.215113\n",
      "450/3150: train loss: 0.249245\n",
      "451/3150: train loss: 0.185341\n",
      "452/3150: train loss: 0.134452\n",
      "453/3150: train loss: 0.115944\n",
      "454/3150: train loss: 0.315355\n",
      "455/3150: train loss: 0.214825\n",
      "456/3150: train loss: 0.179577\n",
      "457/3150: train loss: 0.131998\n",
      "458/3150: train loss: 0.183711\n",
      "459/3150: train loss: 0.300708\n",
      "460/3150: train loss: 0.146758\n",
      "461/3150: train loss: 0.132034\n",
      "462/3150: train loss: 0.160047\n",
      "463/3150: train loss: 0.151837\n",
      "464/3150: train loss: 0.098951\n",
      "465/3150: train loss: 0.110937\n",
      "466/3150: train loss: 0.252736\n",
      "467/3150: train loss: 0.110182\n",
      "468/3150: train loss: 0.148528\n",
      "469/3150: train loss: 0.167388\n",
      "470/3150: train loss: 0.159982\n",
      "471/3150: train loss: 0.115581\n",
      "472/3150: train loss: 0.094827\n",
      "473/3150: train loss: 0.110289\n",
      "474/3150: train loss: 0.180037\n",
      "475/3150: train loss: 0.147141\n",
      "476/3150: train loss: 0.172689\n",
      "477/3150: train loss: 0.106019\n",
      "478/3150: train loss: 0.139841\n",
      "479/3150: train loss: 0.135297\n",
      "480/3150: train loss: 0.106060\n",
      "481/3150: train loss: 0.127683\n",
      "482/3150: train loss: 0.116475\n",
      "483/3150: train loss: 0.137268\n",
      "484/3150: train loss: 0.155065\n",
      "485/3150: train loss: 0.169907\n",
      "486/3150: train loss: 0.188757\n",
      "487/3150: train loss: 0.151878\n",
      "488/3150: train loss: 0.125919\n",
      "489/3150: train loss: 0.078893\n",
      "490/3150: train loss: 0.131626\n",
      "491/3150: train loss: 0.116176\n",
      "492/3150: train loss: 0.142966\n",
      "493/3150: train loss: 0.161052\n",
      "494/3150: train loss: 0.129751\n",
      "495/3150: train loss: 0.119436\n",
      "496/3150: train loss: 0.102404\n",
      "497/3150: train loss: 0.173349\n",
      "498/3150: train loss: 0.155197\n",
      "499/3150: train loss: 0.195832\n",
      "500/3150: train loss: 0.163746\n",
      "501/3150: train loss: 0.220670\n",
      "502/3150: train loss: 0.154040\n",
      "503/3150: train loss: 0.164189\n",
      "504/3150: train loss: 0.110965\n",
      "505/3150: train loss: 0.109189\n",
      "506/3150: train loss: 0.141406\n",
      "507/3150: train loss: 0.175345\n",
      "508/3150: train loss: 0.309624\n",
      "509/3150: train loss: 0.091897\n",
      "510/3150: train loss: 0.148260\n",
      "511/3150: train loss: 0.119197\n",
      "512/3150: train loss: 0.142134\n",
      "513/3150: train loss: 0.151751\n",
      "514/3150: train loss: 0.130320\n",
      "515/3150: train loss: 0.150633\n",
      "516/3150: train loss: 0.144801\n",
      "517/3150: train loss: 0.267502\n",
      "518/3150: train loss: 0.129117\n",
      "519/3150: train loss: 0.097613\n",
      "520/3150: train loss: 0.090487\n",
      "521/3150: train loss: 0.061158\n",
      "522/3150: train loss: 0.178459\n",
      "523/3150: train loss: 0.233821\n",
      "524/3150: train loss: 0.104974\n",
      "525/3150: train loss: 0.155903\n",
      "526/3150: train loss: 0.107038\n",
      "527/3150: train loss: 0.083531\n",
      "528/3150: train loss: 0.164465\n",
      "529/3150: train loss: 0.095472\n",
      "530/3150: train loss: 0.120738\n",
      "531/3150: train loss: 0.126925\n",
      "532/3150: train loss: 0.149163\n",
      "533/3150: train loss: 0.079827\n",
      "534/3150: train loss: 0.152543\n",
      "535/3150: train loss: 0.155697\n",
      "536/3150: train loss: 0.135970\n",
      "537/3150: train loss: 0.245248\n",
      "538/3150: train loss: 0.236142\n",
      "539/3150: train loss: 0.225201\n",
      "540/3150: train loss: 0.150492\n",
      "541/3150: train loss: 0.163167\n",
      "542/3150: train loss: 0.128714\n",
      "543/3150: train loss: 0.197018\n",
      "544/3150: train loss: 0.235984\n",
      "545/3150: train loss: 0.155786\n",
      "546/3150: train loss: 0.061537\n",
      "547/3150: train loss: 0.113104\n",
      "548/3150: train loss: 0.137883\n",
      "549/3150: train loss: 0.115559\n",
      "550/3150: train loss: 0.057697\n",
      "551/3150: train loss: 0.050692\n",
      "552/3150: train loss: 0.090266\n",
      "553/3150: train loss: 0.134553\n",
      "554/3150: train loss: 0.140885\n",
      "555/3150: train loss: 0.121171\n",
      "556/3150: train loss: 0.118997\n",
      "557/3150: train loss: 0.133330\n",
      "558/3150: train loss: 0.090061\n",
      "559/3150: train loss: 0.099577\n",
      "560/3150: train loss: 0.125049\n",
      "561/3150: train loss: 0.191430\n",
      "562/3150: train loss: 0.113316\n",
      "563/3150: train loss: 0.123853\n",
      "564/3150: train loss: 0.153030\n",
      "565/3150: train loss: 0.103519\n",
      "566/3150: train loss: 0.090749\n",
      "567/3150: train loss: 0.067534\n",
      "568/3150: train loss: 0.105380\n",
      "569/3150: train loss: 0.091547\n",
      "570/3150: train loss: 0.169301\n",
      "571/3150: train loss: 0.163559\n",
      "572/3150: train loss: 0.171056\n",
      "573/3150: train loss: 0.104470\n",
      "574/3150: train loss: 0.108861\n",
      "575/3150: train loss: 0.107791\n",
      "576/3150: train loss: 0.196251\n",
      "577/3150: train loss: 0.111548\n",
      "578/3150: train loss: 0.103421\n",
      "579/3150: train loss: 0.120006\n",
      "580/3150: train loss: 0.054216\n",
      "581/3150: train loss: 0.127877\n",
      "582/3150: train loss: 0.104513\n",
      "583/3150: train loss: 0.110807\n",
      "584/3150: train loss: 0.087853\n",
      "585/3150: train loss: 0.103224\n",
      "586/3150: train loss: 0.076932\n",
      "587/3150: train loss: 0.059630\n",
      "588/3150: train loss: 0.076197\n",
      "589/3150: train loss: 0.105738\n",
      "590/3150: train loss: 0.073699\n",
      "591/3150: train loss: 0.149970\n",
      "592/3150: train loss: 0.120217\n",
      "593/3150: train loss: 0.107273\n",
      "594/3150: train loss: 0.075956\n",
      "595/3150: train loss: 0.137751\n",
      "596/3150: train loss: 0.096359\n",
      "597/3150: train loss: 0.127603\n",
      "598/3150: train loss: 0.147851\n",
      "599/3150: train loss: 0.175020\n",
      "600/3150: train loss: 0.172054\n",
      "601/3150: train loss: 0.108192\n",
      "602/3150: train loss: 0.121631\n",
      "603/3150: train loss: 0.060004\n",
      "604/3150: train loss: 0.120105\n",
      "605/3150: train loss: 0.137578\n",
      "606/3150: train loss: 0.087608\n",
      "607/3150: train loss: 0.160103\n",
      "608/3150: train loss: 0.212205\n",
      "609/3150: train loss: 0.147173\n",
      "610/3150: train loss: 0.153465\n",
      "611/3150: train loss: 0.109514\n",
      "612/3150: train loss: 0.127437\n",
      "613/3150: train loss: 0.173414\n",
      "614/3150: train loss: 0.111373\n",
      "615/3150: train loss: 0.124445\n",
      "616/3150: train loss: 0.167659\n",
      "617/3150: train loss: 0.099117\n",
      "618/3150: train loss: 0.098019\n",
      "619/3150: train loss: 0.121977\n",
      "620/3150: train loss: 0.117788\n",
      "621/3150: train loss: 0.104911\n",
      "622/3150: train loss: 0.095385\n",
      "623/3150: train loss: 0.150175\n",
      "624/3150: train loss: 0.126705\n",
      "625/3150: train loss: 0.126254\n",
      "626/3150: train loss: 0.063218\n",
      "627/3150: train loss: 0.222357\n",
      "628/3150: train loss: 0.090322\n",
      "629/3150: train loss: 0.100196\n",
      "(0.9508571428571428, 0.9483809523809524)\n",
      "630/3150: train loss: 0.132084\n",
      "631/3150: train loss: 0.092247\n",
      "632/3150: train loss: 0.129906\n",
      "633/3150: train loss: 0.082516\n",
      "634/3150: train loss: 0.136089\n",
      "635/3150: train loss: 0.095566\n",
      "636/3150: train loss: 0.102460\n",
      "637/3150: train loss: 0.062289\n",
      "638/3150: train loss: 0.145339\n",
      "639/3150: train loss: 0.088862\n",
      "640/3150: train loss: 0.140262\n",
      "641/3150: train loss: 0.061274\n",
      "642/3150: train loss: 0.138833\n",
      "643/3150: train loss: 0.053756\n",
      "644/3150: train loss: 0.128658\n",
      "645/3150: train loss: 0.143516\n",
      "646/3150: train loss: 0.094977\n",
      "647/3150: train loss: 0.101166\n",
      "648/3150: train loss: 0.126327\n",
      "649/3150: train loss: 0.127370\n",
      "650/3150: train loss: 0.079879\n",
      "651/3150: train loss: 0.137097\n",
      "652/3150: train loss: 0.164118\n",
      "653/3150: train loss: 0.080302\n",
      "654/3150: train loss: 0.074967\n",
      "655/3150: train loss: 0.164938\n",
      "656/3150: train loss: 0.056473\n",
      "657/3150: train loss: 0.082011\n",
      "658/3150: train loss: 0.220924\n",
      "659/3150: train loss: 0.105410\n",
      "660/3150: train loss: 0.123879\n",
      "661/3150: train loss: 0.099454\n",
      "662/3150: train loss: 0.114383\n",
      "663/3150: train loss: 0.064737\n",
      "664/3150: train loss: 0.139637\n",
      "665/3150: train loss: 0.156781\n",
      "666/3150: train loss: 0.067204\n",
      "667/3150: train loss: 0.070961\n",
      "668/3150: train loss: 0.122279\n",
      "669/3150: train loss: 0.129094\n",
      "670/3150: train loss: 0.077811\n",
      "671/3150: train loss: 0.088828\n",
      "672/3150: train loss: 0.131403\n",
      "673/3150: train loss: 0.182018\n",
      "674/3150: train loss: 0.143820\n",
      "675/3150: train loss: 0.111815\n",
      "676/3150: train loss: 0.133756\n",
      "677/3150: train loss: 0.091316\n",
      "678/3150: train loss: 0.130921\n",
      "679/3150: train loss: 0.101438\n",
      "680/3150: train loss: 0.133041\n",
      "681/3150: train loss: 0.176162\n",
      "682/3150: train loss: 0.058751\n",
      "683/3150: train loss: 0.088771\n",
      "684/3150: train loss: 0.102373\n",
      "685/3150: train loss: 0.095208\n",
      "686/3150: train loss: 0.114185\n",
      "687/3150: train loss: 0.099464\n",
      "688/3150: train loss: 0.097703\n",
      "689/3150: train loss: 0.148312\n",
      "690/3150: train loss: 0.068798\n",
      "691/3150: train loss: 0.090001\n",
      "692/3150: train loss: 0.099091\n",
      "693/3150: train loss: 0.078688\n",
      "694/3150: train loss: 0.048709\n",
      "695/3150: train loss: 0.095292\n",
      "696/3150: train loss: 0.102045\n",
      "697/3150: train loss: 0.131067\n",
      "698/3150: train loss: 0.099983\n",
      "699/3150: train loss: 0.114857\n",
      "700/3150: train loss: 0.156313\n",
      "701/3150: train loss: 0.078339\n",
      "702/3150: train loss: 0.124507\n",
      "703/3150: train loss: 0.108512\n",
      "704/3150: train loss: 0.064951\n",
      "705/3150: train loss: 0.092383\n",
      "706/3150: train loss: 0.056168\n",
      "707/3150: train loss: 0.092942\n",
      "708/3150: train loss: 0.100601\n",
      "709/3150: train loss: 0.083797\n",
      "710/3150: train loss: 0.046562\n",
      "711/3150: train loss: 0.108810\n",
      "712/3150: train loss: 0.100996\n",
      "713/3150: train loss: 0.093396\n",
      "714/3150: train loss: 0.056781\n",
      "715/3150: train loss: 0.078195\n",
      "716/3150: train loss: 0.121838\n",
      "717/3150: train loss: 0.082506\n",
      "718/3150: train loss: 0.094058\n",
      "719/3150: train loss: 0.141454\n",
      "720/3150: train loss: 0.081648\n",
      "721/3150: train loss: 0.103691\n",
      "722/3150: train loss: 0.087905\n",
      "723/3150: train loss: 0.102874\n",
      "724/3150: train loss: 0.167315\n",
      "725/3150: train loss: 0.092269\n",
      "726/3150: train loss: 0.093157\n",
      "727/3150: train loss: 0.174417\n",
      "728/3150: train loss: 0.065499\n",
      "729/3150: train loss: 0.120080\n",
      "730/3150: train loss: 0.137094\n",
      "731/3150: train loss: 0.075799\n",
      "732/3150: train loss: 0.102968\n",
      "733/3150: train loss: 0.081836\n",
      "734/3150: train loss: 0.071273\n",
      "735/3150: train loss: 0.065466\n",
      "736/3150: train loss: 0.090131\n",
      "737/3150: train loss: 0.168837\n",
      "738/3150: train loss: 0.072534\n",
      "739/3150: train loss: 0.092555\n",
      "740/3150: train loss: 0.119895\n",
      "741/3150: train loss: 0.063728\n",
      "742/3150: train loss: 0.055486\n",
      "743/3150: train loss: 0.188057\n",
      "744/3150: train loss: 0.195815\n",
      "745/3150: train loss: 0.063732\n",
      "746/3150: train loss: 0.052284\n",
      "747/3150: train loss: 0.158239\n",
      "748/3150: train loss: 0.055387\n",
      "749/3150: train loss: 0.076370\n",
      "750/3150: train loss: 0.044175\n",
      "751/3150: train loss: 0.067592\n",
      "752/3150: train loss: 0.170005\n",
      "753/3150: train loss: 0.176109\n",
      "754/3150: train loss: 0.107691\n",
      "755/3150: train loss: 0.218613\n",
      "756/3150: train loss: 0.100789\n",
      "757/3150: train loss: 0.065332\n",
      "758/3150: train loss: 0.055139\n",
      "759/3150: train loss: 0.172454\n",
      "760/3150: train loss: 0.041130\n",
      "761/3150: train loss: 0.167140\n",
      "762/3150: train loss: 0.108867\n",
      "763/3150: train loss: 0.060703\n",
      "764/3150: train loss: 0.126356\n",
      "765/3150: train loss: 0.060378\n",
      "766/3150: train loss: 0.110129\n",
      "767/3150: train loss: 0.121723\n",
      "768/3150: train loss: 0.100405\n",
      "769/3150: train loss: 0.149740\n",
      "770/3150: train loss: 0.107865\n",
      "771/3150: train loss: 0.127227\n",
      "772/3150: train loss: 0.138345\n",
      "773/3150: train loss: 0.115181\n",
      "774/3150: train loss: 0.042843\n",
      "775/3150: train loss: 0.043743\n",
      "776/3150: train loss: 0.086070\n",
      "777/3150: train loss: 0.053019\n",
      "778/3150: train loss: 0.099381\n",
      "779/3150: train loss: 0.026461\n",
      "780/3150: train loss: 0.067858\n",
      "781/3150: train loss: 0.076052\n",
      "782/3150: train loss: 0.105076\n",
      "783/3150: train loss: 0.117356\n",
      "784/3150: train loss: 0.147839\n",
      "785/3150: train loss: 0.037166\n",
      "786/3150: train loss: 0.072951\n",
      "787/3150: train loss: 0.107243\n",
      "788/3150: train loss: 0.110315\n",
      "789/3150: train loss: 0.162648\n",
      "790/3150: train loss: 0.101724\n",
      "791/3150: train loss: 0.138039\n",
      "792/3150: train loss: 0.097238\n",
      "793/3150: train loss: 0.058272\n",
      "794/3150: train loss: 0.107092\n",
      "795/3150: train loss: 0.057536\n",
      "796/3150: train loss: 0.185073\n",
      "797/3150: train loss: 0.107073\n",
      "798/3150: train loss: 0.052270\n",
      "799/3150: train loss: 0.128152\n",
      "800/3150: train loss: 0.052058\n",
      "801/3150: train loss: 0.069425\n",
      "802/3150: train loss: 0.064684\n",
      "803/3150: train loss: 0.045624\n",
      "804/3150: train loss: 0.083718\n",
      "805/3150: train loss: 0.093128\n",
      "806/3150: train loss: 0.054990\n",
      "807/3150: train loss: 0.082082\n",
      "808/3150: train loss: 0.032977\n",
      "809/3150: train loss: 0.134717\n",
      "810/3150: train loss: 0.073736\n",
      "811/3150: train loss: 0.046284\n",
      "812/3150: train loss: 0.182068\n",
      "813/3150: train loss: 0.087297\n",
      "814/3150: train loss: 0.158951\n",
      "815/3150: train loss: 0.083605\n",
      "816/3150: train loss: 0.050186\n",
      "817/3150: train loss: 0.092054\n",
      "818/3150: train loss: 0.097478\n",
      "819/3150: train loss: 0.144217\n",
      "820/3150: train loss: 0.079210\n",
      "821/3150: train loss: 0.075991\n",
      "822/3150: train loss: 0.069086\n",
      "823/3150: train loss: 0.052153\n",
      "824/3150: train loss: 0.105372\n",
      "825/3150: train loss: 0.110625\n",
      "826/3150: train loss: 0.077401\n",
      "827/3150: train loss: 0.097760\n",
      "828/3150: train loss: 0.077882\n",
      "829/3150: train loss: 0.156220\n",
      "830/3150: train loss: 0.079577\n",
      "831/3150: train loss: 0.073425\n",
      "832/3150: train loss: 0.149828\n",
      "833/3150: train loss: 0.148538\n",
      "834/3150: train loss: 0.043683\n",
      "835/3150: train loss: 0.082420\n",
      "836/3150: train loss: 0.074615\n",
      "837/3150: train loss: 0.115812\n",
      "838/3150: train loss: 0.062909\n",
      "839/3150: train loss: 0.070847\n",
      "840/3150: train loss: 0.146364\n",
      "841/3150: train loss: 0.073145\n",
      "842/3150: train loss: 0.046959\n",
      "843/3150: train loss: 0.200421\n",
      "844/3150: train loss: 0.076767\n",
      "845/3150: train loss: 0.079859\n",
      "846/3150: train loss: 0.105186\n",
      "847/3150: train loss: 0.041437\n",
      "848/3150: train loss: 0.090132\n",
      "849/3150: train loss: 0.043290\n",
      "850/3150: train loss: 0.120932\n",
      "851/3150: train loss: 0.137087\n",
      "852/3150: train loss: 0.048529\n",
      "853/3150: train loss: 0.088484\n",
      "854/3150: train loss: 0.053603\n",
      "855/3150: train loss: 0.095127\n",
      "856/3150: train loss: 0.123070\n",
      "857/3150: train loss: 0.070748\n",
      "858/3150: train loss: 0.087111\n",
      "859/3150: train loss: 0.053252\n",
      "860/3150: train loss: 0.043390\n",
      "861/3150: train loss: 0.131565\n",
      "862/3150: train loss: 0.118823\n",
      "863/3150: train loss: 0.139217\n",
      "864/3150: train loss: 0.092158\n",
      "865/3150: train loss: 0.285145\n",
      "866/3150: train loss: 0.094934\n",
      "867/3150: train loss: 0.124553\n",
      "868/3150: train loss: 0.053802\n",
      "869/3150: train loss: 0.147654\n",
      "870/3150: train loss: 0.045244\n",
      "871/3150: train loss: 0.087115\n",
      "872/3150: train loss: 0.058736\n",
      "873/3150: train loss: 0.069631\n",
      "874/3150: train loss: 0.059972\n",
      "875/3150: train loss: 0.091456\n",
      "876/3150: train loss: 0.062894\n",
      "877/3150: train loss: 0.068367\n",
      "878/3150: train loss: 0.060107\n",
      "879/3150: train loss: 0.086156\n",
      "880/3150: train loss: 0.037615\n",
      "881/3150: train loss: 0.032327\n",
      "882/3150: train loss: 0.101960\n",
      "883/3150: train loss: 0.083607\n",
      "884/3150: train loss: 0.117287\n",
      "885/3150: train loss: 0.123152\n",
      "886/3150: train loss: 0.055945\n",
      "887/3150: train loss: 0.117640\n",
      "888/3150: train loss: 0.151789\n",
      "889/3150: train loss: 0.061013\n",
      "890/3150: train loss: 0.030225\n",
      "891/3150: train loss: 0.085932\n",
      "892/3150: train loss: 0.133750\n",
      "893/3150: train loss: 0.061401\n",
      "894/3150: train loss: 0.087513\n",
      "895/3150: train loss: 0.065179\n",
      "896/3150: train loss: 0.090200\n",
      "897/3150: train loss: 0.051359\n",
      "898/3150: train loss: 0.080123\n",
      "899/3150: train loss: 0.096981\n",
      "900/3150: train loss: 0.054150\n",
      "901/3150: train loss: 0.130550\n",
      "902/3150: train loss: 0.066340\n",
      "903/3150: train loss: 0.087395\n",
      "904/3150: train loss: 0.074018\n",
      "905/3150: train loss: 0.045077\n",
      "906/3150: train loss: 0.055724\n",
      "907/3150: train loss: 0.099564\n",
      "908/3150: train loss: 0.061732\n",
      "909/3150: train loss: 0.028060\n",
      "910/3150: train loss: 0.167576\n",
      "911/3150: train loss: 0.054172\n",
      "912/3150: train loss: 0.128211\n",
      "913/3150: train loss: 0.206491\n",
      "914/3150: train loss: 0.123314\n",
      "915/3150: train loss: 0.123294\n",
      "916/3150: train loss: 0.133639\n",
      "917/3150: train loss: 0.044030\n",
      "918/3150: train loss: 0.045133\n",
      "919/3150: train loss: 0.039744\n",
      "920/3150: train loss: 0.118196\n",
      "921/3150: train loss: 0.080496\n",
      "922/3150: train loss: 0.133839\n",
      "923/3150: train loss: 0.065259\n",
      "924/3150: train loss: 0.074971\n",
      "925/3150: train loss: 0.070194\n",
      "926/3150: train loss: 0.077126\n",
      "927/3150: train loss: 0.048383\n",
      "928/3150: train loss: 0.091906\n",
      "929/3150: train loss: 0.035405\n",
      "930/3150: train loss: 0.037340\n",
      "931/3150: train loss: 0.051375\n",
      "932/3150: train loss: 0.052495\n",
      "933/3150: train loss: 0.115530\n",
      "934/3150: train loss: 0.028483\n",
      "935/3150: train loss: 0.055408\n",
      "936/3150: train loss: 0.059636\n",
      "937/3150: train loss: 0.060364\n",
      "938/3150: train loss: 0.102306\n",
      "939/3150: train loss: 0.047871\n",
      "940/3150: train loss: 0.111963\n",
      "941/3150: train loss: 0.075779\n",
      "942/3150: train loss: 0.093253\n",
      "943/3150: train loss: 0.048615\n",
      "944/3150: train loss: 0.062277\n",
      "(0.9632380952380952, 0.960952380952381)\n",
      "945/3150: train loss: 0.069422\n",
      "946/3150: train loss: 0.061364\n",
      "947/3150: train loss: 0.072371\n",
      "948/3150: train loss: 0.121149\n",
      "949/3150: train loss: 0.052268\n",
      "950/3150: train loss: 0.061846\n",
      "951/3150: train loss: 0.169552\n",
      "952/3150: train loss: 0.096992\n",
      "953/3150: train loss: 0.067892\n",
      "954/3150: train loss: 0.075709\n",
      "955/3150: train loss: 0.050107\n",
      "956/3150: train loss: 0.104007\n",
      "957/3150: train loss: 0.035852\n",
      "958/3150: train loss: 0.058253\n",
      "959/3150: train loss: 0.049342\n",
      "960/3150: train loss: 0.075417\n",
      "961/3150: train loss: 0.061263\n",
      "962/3150: train loss: 0.087246\n",
      "963/3150: train loss: 0.130965\n",
      "964/3150: train loss: 0.072086\n",
      "965/3150: train loss: 0.071756\n",
      "966/3150: train loss: 0.041755\n",
      "967/3150: train loss: 0.075838\n",
      "968/3150: train loss: 0.159042\n",
      "969/3150: train loss: 0.098001\n",
      "970/3150: train loss: 0.150628\n",
      "971/3150: train loss: 0.056262\n",
      "972/3150: train loss: 0.063820\n",
      "973/3150: train loss: 0.100546\n",
      "974/3150: train loss: 0.096880\n",
      "975/3150: train loss: 0.055935\n",
      "976/3150: train loss: 0.114143\n",
      "977/3150: train loss: 0.097998\n",
      "978/3150: train loss: 0.101404\n",
      "979/3150: train loss: 0.097736\n",
      "980/3150: train loss: 0.067078\n",
      "981/3150: train loss: 0.094709\n",
      "982/3150: train loss: 0.079739\n",
      "983/3150: train loss: 0.089086\n",
      "984/3150: train loss: 0.054562\n",
      "985/3150: train loss: 0.086004\n",
      "986/3150: train loss: 0.060930\n",
      "987/3150: train loss: 0.034757\n",
      "988/3150: train loss: 0.129027\n",
      "989/3150: train loss: 0.057232\n",
      "990/3150: train loss: 0.065334\n",
      "991/3150: train loss: 0.073399\n",
      "992/3150: train loss: 0.117366\n",
      "993/3150: train loss: 0.066100\n",
      "994/3150: train loss: 0.038169\n",
      "995/3150: train loss: 0.039083\n",
      "996/3150: train loss: 0.049100\n",
      "997/3150: train loss: 0.085399\n",
      "998/3150: train loss: 0.046449\n",
      "999/3150: train loss: 0.106578\n",
      "1000/3150: train loss: 0.083279\n",
      "1001/3150: train loss: 0.047808\n",
      "1002/3150: train loss: 0.024939\n",
      "1003/3150: train loss: 0.053666\n",
      "1004/3150: train loss: 0.084910\n",
      "1005/3150: train loss: 0.051308\n",
      "1006/3150: train loss: 0.078564\n",
      "1007/3150: train loss: 0.045515\n",
      "1008/3150: train loss: 0.027393\n",
      "1009/3150: train loss: 0.050963\n",
      "1010/3150: train loss: 0.054145\n",
      "1011/3150: train loss: 0.040853\n",
      "1012/3150: train loss: 0.060171\n",
      "1013/3150: train loss: 0.066181\n",
      "1014/3150: train loss: 0.062891\n",
      "1015/3150: train loss: 0.136611\n",
      "1016/3150: train loss: 0.093401\n",
      "1017/3150: train loss: 0.128415\n",
      "1018/3150: train loss: 0.058720\n",
      "1019/3150: train loss: 0.073354\n",
      "1020/3150: train loss: 0.045329\n",
      "1021/3150: train loss: 0.074122\n",
      "1022/3150: train loss: 0.111296\n",
      "1023/3150: train loss: 0.073530\n",
      "1024/3150: train loss: 0.057237\n",
      "1025/3150: train loss: 0.045890\n",
      "1026/3150: train loss: 0.049263\n",
      "1027/3150: train loss: 0.113047\n",
      "1028/3150: train loss: 0.049812\n",
      "1029/3150: train loss: 0.076363\n",
      "1030/3150: train loss: 0.047267\n",
      "1031/3150: train loss: 0.039829\n",
      "1032/3150: train loss: 0.038266\n",
      "1033/3150: train loss: 0.069113\n",
      "1034/3150: train loss: 0.079208\n",
      "1035/3150: train loss: 0.065494\n",
      "1036/3150: train loss: 0.098854\n",
      "1037/3150: train loss: 0.066894\n",
      "1038/3150: train loss: 0.107471\n",
      "1039/3150: train loss: 0.069347\n",
      "1040/3150: train loss: 0.029000\n",
      "1041/3150: train loss: 0.062418\n",
      "1042/3150: train loss: 0.055348\n",
      "1043/3150: train loss: 0.111705\n",
      "1044/3150: train loss: 0.043430\n",
      "1045/3150: train loss: 0.060811\n",
      "1046/3150: train loss: 0.035979\n",
      "1047/3150: train loss: 0.098787\n",
      "1048/3150: train loss: 0.129322\n",
      "1049/3150: train loss: 0.043473\n",
      "1050/3150: train loss: 0.103715\n",
      "1051/3150: train loss: 0.221361\n",
      "1052/3150: train loss: 0.059477\n",
      "1053/3150: train loss: 0.054773\n",
      "1054/3150: train loss: 0.052750\n",
      "1055/3150: train loss: 0.079515\n",
      "1056/3150: train loss: 0.080406\n",
      "1057/3150: train loss: 0.021602\n",
      "1058/3150: train loss: 0.060667\n",
      "1059/3150: train loss: 0.084429\n",
      "1060/3150: train loss: 0.083412\n",
      "1061/3150: train loss: 0.100189\n",
      "1062/3150: train loss: 0.058174\n",
      "1063/3150: train loss: 0.066426\n",
      "1064/3150: train loss: 0.155982\n",
      "1065/3150: train loss: 0.085971\n",
      "1066/3150: train loss: 0.069344\n",
      "1067/3150: train loss: 0.078869\n",
      "1068/3150: train loss: 0.106205\n",
      "1069/3150: train loss: 0.065949\n",
      "1070/3150: train loss: 0.134049\n",
      "1071/3150: train loss: 0.047574\n",
      "1072/3150: train loss: 0.072909\n",
      "1073/3150: train loss: 0.062441\n",
      "1074/3150: train loss: 0.067918\n",
      "1075/3150: train loss: 0.094157\n",
      "1076/3150: train loss: 0.122940\n",
      "1077/3150: train loss: 0.035429\n",
      "1078/3150: train loss: 0.111502\n",
      "1079/3150: train loss: 0.105816\n",
      "1080/3150: train loss: 0.113858\n",
      "1081/3150: train loss: 0.074601\n",
      "1082/3150: train loss: 0.032214\n",
      "1083/3150: train loss: 0.089948\n",
      "1084/3150: train loss: 0.046147\n",
      "1085/3150: train loss: 0.031725\n",
      "1086/3150: train loss: 0.052081\n",
      "1087/3150: train loss: 0.042525\n",
      "1088/3150: train loss: 0.071637\n",
      "1089/3150: train loss: 0.069958\n",
      "1090/3150: train loss: 0.122554\n",
      "1091/3150: train loss: 0.093122\n",
      "1092/3150: train loss: 0.060323\n",
      "1093/3150: train loss: 0.045342\n",
      "1094/3150: train loss: 0.057149\n",
      "1095/3150: train loss: 0.097897\n",
      "1096/3150: train loss: 0.064890\n",
      "1097/3150: train loss: 0.054985\n",
      "1098/3150: train loss: 0.046746\n",
      "1099/3150: train loss: 0.053968\n",
      "1100/3150: train loss: 0.102461\n",
      "1101/3150: train loss: 0.066340\n",
      "1102/3150: train loss: 0.067681\n",
      "1103/3150: train loss: 0.134235\n",
      "1104/3150: train loss: 0.068738\n",
      "1105/3150: train loss: 0.065951\n",
      "1106/3150: train loss: 0.068010\n",
      "1107/3150: train loss: 0.036062\n",
      "1108/3150: train loss: 0.069105\n",
      "1109/3150: train loss: 0.072061\n",
      "1110/3150: train loss: 0.110814\n",
      "1111/3150: train loss: 0.041165\n",
      "1112/3150: train loss: 0.056653\n",
      "1113/3150: train loss: 0.055655\n",
      "1114/3150: train loss: 0.121480\n",
      "1115/3150: train loss: 0.063004\n",
      "1116/3150: train loss: 0.056029\n",
      "1117/3150: train loss: 0.073414\n",
      "1118/3150: train loss: 0.044509\n",
      "1119/3150: train loss: 0.049916\n",
      "1120/3150: train loss: 0.084515\n",
      "1121/3150: train loss: 0.094620\n",
      "1122/3150: train loss: 0.077759\n",
      "1123/3150: train loss: 0.100098\n",
      "1124/3150: train loss: 0.089434\n",
      "1125/3150: train loss: 0.046752\n",
      "1126/3150: train loss: 0.063613\n",
      "1127/3150: train loss: 0.071801\n",
      "1128/3150: train loss: 0.045392\n",
      "1129/3150: train loss: 0.081310\n",
      "1130/3150: train loss: 0.100302\n",
      "1131/3150: train loss: 0.092487\n",
      "1132/3150: train loss: 0.051076\n",
      "1133/3150: train loss: 0.106000\n",
      "1134/3150: train loss: 0.079467\n",
      "1135/3150: train loss: 0.051216\n",
      "1136/3150: train loss: 0.122920\n",
      "1137/3150: train loss: 0.053213\n",
      "1138/3150: train loss: 0.103008\n",
      "1139/3150: train loss: 0.042445\n",
      "1140/3150: train loss: 0.040041\n",
      "1141/3150: train loss: 0.122235\n",
      "1142/3150: train loss: 0.045334\n",
      "1143/3150: train loss: 0.086470\n",
      "1144/3150: train loss: 0.037652\n",
      "1145/3150: train loss: 0.043760\n",
      "1146/3150: train loss: 0.028440\n",
      "1147/3150: train loss: 0.092345\n",
      "1148/3150: train loss: 0.088540\n",
      "1149/3150: train loss: 0.076619\n",
      "1150/3150: train loss: 0.075539\n",
      "1151/3150: train loss: 0.069733\n",
      "1152/3150: train loss: 0.055840\n",
      "1153/3150: train loss: 0.109360\n",
      "1154/3150: train loss: 0.050083\n",
      "1155/3150: train loss: 0.053783\n",
      "1156/3150: train loss: 0.065504\n",
      "1157/3150: train loss: 0.027874\n",
      "1158/3150: train loss: 0.072418\n",
      "1159/3150: train loss: 0.081924\n",
      "1160/3150: train loss: 0.058965\n",
      "1161/3150: train loss: 0.044025\n",
      "1162/3150: train loss: 0.054296\n",
      "1163/3150: train loss: 0.041493\n",
      "1164/3150: train loss: 0.129351\n",
      "1165/3150: train loss: 0.033088\n",
      "1166/3150: train loss: 0.076251\n",
      "1167/3150: train loss: 0.075615\n",
      "1168/3150: train loss: 0.098137\n",
      "1169/3150: train loss: 0.124116\n",
      "1170/3150: train loss: 0.064125\n",
      "1171/3150: train loss: 0.076326\n",
      "1172/3150: train loss: 0.033630\n",
      "1173/3150: train loss: 0.046493\n",
      "1174/3150: train loss: 0.078898\n",
      "1175/3150: train loss: 0.024268\n",
      "1176/3150: train loss: 0.052755\n",
      "1177/3150: train loss: 0.098129\n",
      "1178/3150: train loss: 0.045467\n",
      "1179/3150: train loss: 0.061332\n",
      "1180/3150: train loss: 0.044933\n",
      "1181/3150: train loss: 0.048352\n",
      "1182/3150: train loss: 0.162512\n",
      "1183/3150: train loss: 0.057013\n",
      "1184/3150: train loss: 0.041076\n",
      "1185/3150: train loss: 0.048969\n",
      "1186/3150: train loss: 0.071826\n",
      "1187/3150: train loss: 0.062705\n",
      "1188/3150: train loss: 0.068818\n",
      "1189/3150: train loss: 0.058639\n",
      "1190/3150: train loss: 0.064358\n",
      "1191/3150: train loss: 0.050521\n",
      "1192/3150: train loss: 0.038229\n",
      "1193/3150: train loss: 0.084195\n",
      "1194/3150: train loss: 0.028810\n",
      "1195/3150: train loss: 0.056081\n",
      "1196/3150: train loss: 0.036702\n",
      "1197/3150: train loss: 0.041691\n",
      "1198/3150: train loss: 0.040881\n",
      "1199/3150: train loss: 0.052852\n",
      "1200/3150: train loss: 0.040540\n",
      "1201/3150: train loss: 0.033420\n",
      "1202/3150: train loss: 0.060234\n",
      "1203/3150: train loss: 0.088195\n",
      "1204/3150: train loss: 0.032361\n",
      "1205/3150: train loss: 0.028971\n",
      "1206/3150: train loss: 0.029967\n",
      "1207/3150: train loss: 0.064672\n",
      "1208/3150: train loss: 0.032807\n",
      "1209/3150: train loss: 0.055739\n",
      "1210/3150: train loss: 0.054645\n",
      "1211/3150: train loss: 0.042071\n",
      "1212/3150: train loss: 0.065839\n",
      "1213/3150: train loss: 0.099766\n",
      "1214/3150: train loss: 0.054520\n",
      "1215/3150: train loss: 0.069996\n",
      "1216/3150: train loss: 0.039245\n",
      "1217/3150: train loss: 0.045580\n",
      "1218/3150: train loss: 0.118667\n",
      "1219/3150: train loss: 0.055443\n",
      "1220/3150: train loss: 0.029843\n",
      "1221/3150: train loss: 0.095028\n",
      "1222/3150: train loss: 0.045841\n",
      "1223/3150: train loss: 0.129353\n",
      "1224/3150: train loss: 0.054718\n",
      "1225/3150: train loss: 0.178957\n",
      "1226/3150: train loss: 0.097708\n",
      "1227/3150: train loss: 0.101864\n",
      "1228/3150: train loss: 0.104394\n",
      "1229/3150: train loss: 0.066957\n",
      "1230/3150: train loss: 0.041025\n",
      "1231/3150: train loss: 0.069579\n",
      "1232/3150: train loss: 0.041222\n",
      "1233/3150: train loss: 0.056475\n",
      "1234/3150: train loss: 0.048220\n",
      "1235/3150: train loss: 0.033913\n",
      "1236/3150: train loss: 0.034938\n",
      "1237/3150: train loss: 0.092863\n",
      "1238/3150: train loss: 0.156540\n",
      "1239/3150: train loss: 0.044154\n",
      "1240/3150: train loss: 0.069129\n",
      "1241/3150: train loss: 0.053094\n",
      "1242/3150: train loss: 0.071700\n",
      "1243/3150: train loss: 0.026564\n",
      "1244/3150: train loss: 0.099459\n",
      "1245/3150: train loss: 0.085754\n",
      "1246/3150: train loss: 0.057141\n",
      "1247/3150: train loss: 0.034909\n",
      "1248/3150: train loss: 0.052255\n",
      "1249/3150: train loss: 0.065014\n",
      "1250/3150: train loss: 0.042125\n",
      "1251/3150: train loss: 0.049451\n",
      "1252/3150: train loss: 0.070446\n",
      "1253/3150: train loss: 0.111555\n",
      "1254/3150: train loss: 0.050711\n",
      "1255/3150: train loss: 0.040977\n",
      "1256/3150: train loss: 0.063553\n",
      "1257/3150: train loss: 0.063544\n",
      "1258/3150: train loss: 0.041993\n",
      "1259/3150: train loss: 0.029838\n",
      "(0.9735873015873016, 0.9694285714285714)\n",
      "1260/3150: train loss: 0.031080\n",
      "1261/3150: train loss: 0.080736\n",
      "1262/3150: train loss: 0.072200\n",
      "1263/3150: train loss: 0.063754\n",
      "1264/3150: train loss: 0.030986\n",
      "1265/3150: train loss: 0.072439\n",
      "1266/3150: train loss: 0.036304\n",
      "1267/3150: train loss: 0.035250\n",
      "1268/3150: train loss: 0.056773\n",
      "1269/3150: train loss: 0.095418\n",
      "1270/3150: train loss: 0.039520\n",
      "1271/3150: train loss: 0.091318\n",
      "1272/3150: train loss: 0.037417\n",
      "1273/3150: train loss: 0.072872\n",
      "1274/3150: train loss: 0.031786\n",
      "1275/3150: train loss: 0.077037\n",
      "1276/3150: train loss: 0.155083\n",
      "1277/3150: train loss: 0.071968\n",
      "1278/3150: train loss: 0.035174\n",
      "1279/3150: train loss: 0.029992\n",
      "1280/3150: train loss: 0.043850\n",
      "1281/3150: train loss: 0.071349\n",
      "1282/3150: train loss: 0.051934\n",
      "1283/3150: train loss: 0.042077\n",
      "1284/3150: train loss: 0.036506\n",
      "1285/3150: train loss: 0.028087\n",
      "1286/3150: train loss: 0.046424\n",
      "1287/3150: train loss: 0.115898\n",
      "1288/3150: train loss: 0.066233\n",
      "1289/3150: train loss: 0.030803\n",
      "1290/3150: train loss: 0.040048\n",
      "1291/3150: train loss: 0.077080\n",
      "1292/3150: train loss: 0.072037\n",
      "1293/3150: train loss: 0.037809\n",
      "1294/3150: train loss: 0.051203\n",
      "1295/3150: train loss: 0.040373\n",
      "1296/3150: train loss: 0.111773\n",
      "1297/3150: train loss: 0.073628\n",
      "1298/3150: train loss: 0.132931\n",
      "1299/3150: train loss: 0.030679\n",
      "1300/3150: train loss: 0.061959\n",
      "1301/3150: train loss: 0.082502\n",
      "1302/3150: train loss: 0.089777\n",
      "1303/3150: train loss: 0.020219\n",
      "1304/3150: train loss: 0.094755\n",
      "1305/3150: train loss: 0.081114\n",
      "1306/3150: train loss: 0.046615\n",
      "1307/3150: train loss: 0.072049\n",
      "1308/3150: train loss: 0.047974\n",
      "1309/3150: train loss: 0.047518\n",
      "1310/3150: train loss: 0.085017\n",
      "1311/3150: train loss: 0.063471\n",
      "1312/3150: train loss: 0.051414\n",
      "1313/3150: train loss: 0.049910\n",
      "1314/3150: train loss: 0.078572\n",
      "1315/3150: train loss: 0.181387\n",
      "1316/3150: train loss: 0.027240\n",
      "1317/3150: train loss: 0.073519\n",
      "1318/3150: train loss: 0.063074\n",
      "1319/3150: train loss: 0.062708\n",
      "1320/3150: train loss: 0.048807\n",
      "1321/3150: train loss: 0.030085\n",
      "1322/3150: train loss: 0.054715\n",
      "1323/3150: train loss: 0.099830\n",
      "1324/3150: train loss: 0.034287\n",
      "1325/3150: train loss: 0.028731\n",
      "1326/3150: train loss: 0.063837\n",
      "1327/3150: train loss: 0.046185\n",
      "1328/3150: train loss: 0.046871\n",
      "1329/3150: train loss: 0.067714\n",
      "1330/3150: train loss: 0.185763\n",
      "1331/3150: train loss: 0.048980\n",
      "1332/3150: train loss: 0.037997\n",
      "1333/3150: train loss: 0.107786\n",
      "1334/3150: train loss: 0.031716\n",
      "1335/3150: train loss: 0.069828\n",
      "1336/3150: train loss: 0.027191\n",
      "1337/3150: train loss: 0.036395\n",
      "1338/3150: train loss: 0.134739\n",
      "1339/3150: train loss: 0.075547\n",
      "1340/3150: train loss: 0.039094\n",
      "1341/3150: train loss: 0.126035\n",
      "1342/3150: train loss: 0.033382\n",
      "1343/3150: train loss: 0.078388\n",
      "1344/3150: train loss: 0.020200\n",
      "1345/3150: train loss: 0.064156\n",
      "1346/3150: train loss: 0.037949\n",
      "1347/3150: train loss: 0.044211\n",
      "1348/3150: train loss: 0.061090\n",
      "1349/3150: train loss: 0.043135\n",
      "1350/3150: train loss: 0.100630\n",
      "1351/3150: train loss: 0.063449\n",
      "1352/3150: train loss: 0.177292\n",
      "1353/3150: train loss: 0.062602\n",
      "1354/3150: train loss: 0.049465\n",
      "1355/3150: train loss: 0.042546\n",
      "1356/3150: train loss: 0.050720\n",
      "1357/3150: train loss: 0.069384\n",
      "1358/3150: train loss: 0.032323\n",
      "1359/3150: train loss: 0.045520\n",
      "1360/3150: train loss: 0.028651\n",
      "1361/3150: train loss: 0.028686\n",
      "1362/3150: train loss: 0.074636\n",
      "1363/3150: train loss: 0.069443\n",
      "1364/3150: train loss: 0.105916\n",
      "1365/3150: train loss: 0.018369\n",
      "1366/3150: train loss: 0.110782\n",
      "1367/3150: train loss: 0.041080\n",
      "1368/3150: train loss: 0.061496\n",
      "1369/3150: train loss: 0.051523\n",
      "1370/3150: train loss: 0.090587\n",
      "1371/3150: train loss: 0.061263\n",
      "1372/3150: train loss: 0.062010\n",
      "1373/3150: train loss: 0.152894\n",
      "1374/3150: train loss: 0.113647\n",
      "1375/3150: train loss: 0.039711\n",
      "1376/3150: train loss: 0.047284\n",
      "1377/3150: train loss: 0.069521\n",
      "1378/3150: train loss: 0.065262\n",
      "1379/3150: train loss: 0.092799\n",
      "1380/3150: train loss: 0.041662\n",
      "1381/3150: train loss: 0.069044\n",
      "1382/3150: train loss: 0.086837\n",
      "1383/3150: train loss: 0.028800\n",
      "1384/3150: train loss: 0.058990\n",
      "1385/3150: train loss: 0.071014\n",
      "1386/3150: train loss: 0.043685\n",
      "1387/3150: train loss: 0.032789\n",
      "1388/3150: train loss: 0.056365\n",
      "1389/3150: train loss: 0.038618\n",
      "1390/3150: train loss: 0.045532\n",
      "1391/3150: train loss: 0.068848\n",
      "1392/3150: train loss: 0.039743\n",
      "1393/3150: train loss: 0.037725\n",
      "1394/3150: train loss: 0.091716\n",
      "1395/3150: train loss: 0.081915\n",
      "1396/3150: train loss: 0.058665\n",
      "1397/3150: train loss: 0.035159\n",
      "1398/3150: train loss: 0.057423\n",
      "1399/3150: train loss: 0.043790\n",
      "1400/3150: train loss: 0.043717\n",
      "1401/3150: train loss: 0.091161\n",
      "1402/3150: train loss: 0.033870\n",
      "1403/3150: train loss: 0.065994\n",
      "1404/3150: train loss: 0.042649\n",
      "1405/3150: train loss: 0.076191\n",
      "1406/3150: train loss: 0.025685\n",
      "1407/3150: train loss: 0.062017\n",
      "1408/3150: train loss: 0.031964\n",
      "1409/3150: train loss: 0.088324\n",
      "1410/3150: train loss: 0.068158\n",
      "1411/3150: train loss: 0.022017\n",
      "1412/3150: train loss: 0.030998\n",
      "1413/3150: train loss: 0.118535\n",
      "1414/3150: train loss: 0.037467\n",
      "1415/3150: train loss: 0.030550\n",
      "1416/3150: train loss: 0.051565\n",
      "1417/3150: train loss: 0.034603\n",
      "1418/3150: train loss: 0.047659\n",
      "1419/3150: train loss: 0.056891\n",
      "1420/3150: train loss: 0.028511\n",
      "1421/3150: train loss: 0.029188\n",
      "1422/3150: train loss: 0.106298\n",
      "1423/3150: train loss: 0.068232\n",
      "1424/3150: train loss: 0.048194\n",
      "1425/3150: train loss: 0.043038\n",
      "1426/3150: train loss: 0.106950\n",
      "1427/3150: train loss: 0.029365\n",
      "1428/3150: train loss: 0.032437\n",
      "1429/3150: train loss: 0.041851\n",
      "1430/3150: train loss: 0.080439\n",
      "1431/3150: train loss: 0.076021\n",
      "1432/3150: train loss: 0.095048\n",
      "1433/3150: train loss: 0.048361\n",
      "1434/3150: train loss: 0.095634\n",
      "1435/3150: train loss: 0.060891\n",
      "1436/3150: train loss: 0.014191\n",
      "1437/3150: train loss: 0.025083\n",
      "1438/3150: train loss: 0.052548\n",
      "1439/3150: train loss: 0.043008\n",
      "1440/3150: train loss: 0.054789\n",
      "1441/3150: train loss: 0.082036\n",
      "1442/3150: train loss: 0.019964\n",
      "1443/3150: train loss: 0.047461\n",
      "1444/3150: train loss: 0.038309\n",
      "1445/3150: train loss: 0.056902\n",
      "1446/3150: train loss: 0.030810\n",
      "1447/3150: train loss: 0.038388\n",
      "1448/3150: train loss: 0.037864\n",
      "1449/3150: train loss: 0.115579\n",
      "1450/3150: train loss: 0.048791\n",
      "1451/3150: train loss: 0.059073\n",
      "1452/3150: train loss: 0.106669\n",
      "1453/3150: train loss: 0.062050\n",
      "1454/3150: train loss: 0.066044\n",
      "1455/3150: train loss: 0.039657\n",
      "1456/3150: train loss: 0.106437\n",
      "1457/3150: train loss: 0.039970\n",
      "1458/3150: train loss: 0.032225\n",
      "1459/3150: train loss: 0.045676\n",
      "1460/3150: train loss: 0.055270\n",
      "1461/3150: train loss: 0.030206\n",
      "1462/3150: train loss: 0.063471\n",
      "1463/3150: train loss: 0.083213\n",
      "1464/3150: train loss: 0.036110\n",
      "1465/3150: train loss: 0.072416\n",
      "1466/3150: train loss: 0.019188\n",
      "1467/3150: train loss: 0.045982\n",
      "1468/3150: train loss: 0.072691\n",
      "1469/3150: train loss: 0.071762\n",
      "1470/3150: train loss: 0.048399\n",
      "1471/3150: train loss: 0.024821\n",
      "1472/3150: train loss: 0.070559\n",
      "1473/3150: train loss: 0.080946\n",
      "1474/3150: train loss: 0.069532\n",
      "1475/3150: train loss: 0.067255\n",
      "1476/3150: train loss: 0.105433\n",
      "1477/3150: train loss: 0.067135\n",
      "1478/3150: train loss: 0.026812\n",
      "1479/3150: train loss: 0.032609\n",
      "1480/3150: train loss: 0.036774\n",
      "1481/3150: train loss: 0.044319\n",
      "1482/3150: train loss: 0.044264\n",
      "1483/3150: train loss: 0.039022\n",
      "1484/3150: train loss: 0.094625\n",
      "1485/3150: train loss: 0.084159\n",
      "1486/3150: train loss: 0.034632\n",
      "1487/3150: train loss: 0.016816\n",
      "1488/3150: train loss: 0.010366\n",
      "1489/3150: train loss: 0.053832\n",
      "1490/3150: train loss: 0.059896\n",
      "1491/3150: train loss: 0.053273\n",
      "1492/3150: train loss: 0.051635\n",
      "1493/3150: train loss: 0.038685\n",
      "1494/3150: train loss: 0.031585\n",
      "1495/3150: train loss: 0.067044\n",
      "1496/3150: train loss: 0.027570\n",
      "1497/3150: train loss: 0.043476\n",
      "1498/3150: train loss: 0.025592\n",
      "1499/3150: train loss: 0.037297\n",
      "1500/3150: train loss: 0.111718\n",
      "1501/3150: train loss: 0.043761\n",
      "1502/3150: train loss: 0.023438\n",
      "1503/3150: train loss: 0.034560\n",
      "1504/3150: train loss: 0.098327\n",
      "1505/3150: train loss: 0.047512\n",
      "1506/3150: train loss: 0.026779\n",
      "1507/3150: train loss: 0.062468\n",
      "1508/3150: train loss: 0.066466\n",
      "1509/3150: train loss: 0.030053\n",
      "1510/3150: train loss: 0.042585\n",
      "1511/3150: train loss: 0.024990\n",
      "1512/3150: train loss: 0.020863\n",
      "1513/3150: train loss: 0.097376\n",
      "1514/3150: train loss: 0.016394\n",
      "1515/3150: train loss: 0.040022\n",
      "1516/3150: train loss: 0.080764\n",
      "1517/3150: train loss: 0.079346\n",
      "1518/3150: train loss: 0.032874\n",
      "1519/3150: train loss: 0.059945\n",
      "1520/3150: train loss: 0.051285\n",
      "1521/3150: train loss: 0.027760\n",
      "1522/3150: train loss: 0.099573\n",
      "1523/3150: train loss: 0.054300\n",
      "1524/3150: train loss: 0.039116\n",
      "1525/3150: train loss: 0.027297\n",
      "1526/3150: train loss: 0.074283\n",
      "1527/3150: train loss: 0.035499\n",
      "1528/3150: train loss: 0.039739\n",
      "1529/3150: train loss: 0.075971\n",
      "1530/3150: train loss: 0.068105\n",
      "1531/3150: train loss: 0.080502\n",
      "1532/3150: train loss: 0.126396\n",
      "1533/3150: train loss: 0.039012\n",
      "1534/3150: train loss: 0.030780\n",
      "1535/3150: train loss: 0.018995\n",
      "1536/3150: train loss: 0.049274\n",
      "1537/3150: train loss: 0.039819\n",
      "1538/3150: train loss: 0.037450\n",
      "1539/3150: train loss: 0.034815\n",
      "1540/3150: train loss: 0.062983\n",
      "1541/3150: train loss: 0.031843\n",
      "1542/3150: train loss: 0.049187\n",
      "1543/3150: train loss: 0.053112\n",
      "1544/3150: train loss: 0.033812\n",
      "1545/3150: train loss: 0.056015\n",
      "1546/3150: train loss: 0.051797\n",
      "1547/3150: train loss: 0.048791\n",
      "1548/3150: train loss: 0.048158\n",
      "1549/3150: train loss: 0.038528\n",
      "1550/3150: train loss: 0.020106\n",
      "1551/3150: train loss: 0.021079\n",
      "1552/3150: train loss: 0.047709\n",
      "1553/3150: train loss: 0.058213\n",
      "1554/3150: train loss: 0.056851\n",
      "1555/3150: train loss: 0.026653\n",
      "1556/3150: train loss: 0.043796\n",
      "1557/3150: train loss: 0.048751\n",
      "1558/3150: train loss: 0.081346\n",
      "1559/3150: train loss: 0.017509\n",
      "1560/3150: train loss: 0.038271\n",
      "1561/3150: train loss: 0.048785\n",
      "1562/3150: train loss: 0.049097\n",
      "1563/3150: train loss: 0.142051\n",
      "1564/3150: train loss: 0.015095\n",
      "1565/3150: train loss: 0.025344\n",
      "1566/3150: train loss: 0.064437\n",
      "1567/3150: train loss: 0.048879\n",
      "1568/3150: train loss: 0.080438\n",
      "1569/3150: train loss: 0.022589\n",
      "1570/3150: train loss: 0.031781\n",
      "1571/3150: train loss: 0.050713\n",
      "1572/3150: train loss: 0.068893\n",
      "1573/3150: train loss: 0.025494\n",
      "1574/3150: train loss: 0.054072\n",
      "(0.9771746031746031, 0.973047619047619)\n",
      "1575/3150: train loss: 0.059299\n",
      "1576/3150: train loss: 0.091428\n",
      "1577/3150: train loss: 0.031915\n",
      "1578/3150: train loss: 0.032165\n",
      "1579/3150: train loss: 0.079224\n",
      "1580/3150: train loss: 0.082992\n",
      "1581/3150: train loss: 0.055508\n",
      "1582/3150: train loss: 0.021469\n",
      "1583/3150: train loss: 0.059623\n",
      "1584/3150: train loss: 0.067605\n",
      "1585/3150: train loss: 0.038803\n",
      "1586/3150: train loss: 0.073725\n",
      "1587/3150: train loss: 0.061546\n",
      "1588/3150: train loss: 0.026477\n",
      "1589/3150: train loss: 0.027349\n",
      "1590/3150: train loss: 0.057457\n",
      "1591/3150: train loss: 0.033087\n",
      "1592/3150: train loss: 0.059902\n",
      "1593/3150: train loss: 0.065565\n",
      "1594/3150: train loss: 0.023667\n",
      "1595/3150: train loss: 0.033967\n",
      "1596/3150: train loss: 0.038258\n",
      "1597/3150: train loss: 0.081785\n",
      "1598/3150: train loss: 0.028742\n",
      "1599/3150: train loss: 0.044822\n",
      "1600/3150: train loss: 0.022725\n",
      "1601/3150: train loss: 0.101269\n",
      "1602/3150: train loss: 0.023981\n",
      "1603/3150: train loss: 0.053143\n",
      "1604/3150: train loss: 0.027639\n",
      "1605/3150: train loss: 0.030893\n",
      "1606/3150: train loss: 0.049801\n",
      "1607/3150: train loss: 0.063858\n",
      "1608/3150: train loss: 0.014818\n",
      "1609/3150: train loss: 0.045897\n",
      "1610/3150: train loss: 0.050503\n",
      "1611/3150: train loss: 0.050596\n",
      "1612/3150: train loss: 0.045439\n",
      "1613/3150: train loss: 0.097718\n",
      "1614/3150: train loss: 0.021558\n",
      "1615/3150: train loss: 0.033897\n",
      "1616/3150: train loss: 0.073974\n",
      "1617/3150: train loss: 0.076653\n",
      "1618/3150: train loss: 0.048534\n",
      "1619/3150: train loss: 0.035059\n",
      "1620/3150: train loss: 0.025687\n",
      "1621/3150: train loss: 0.039785\n",
      "1622/3150: train loss: 0.067016\n",
      "1623/3150: train loss: 0.037372\n",
      "1624/3150: train loss: 0.022632\n",
      "1625/3150: train loss: 0.026776\n",
      "1626/3150: train loss: 0.030958\n",
      "1627/3150: train loss: 0.046259\n",
      "1628/3150: train loss: 0.028716\n",
      "1629/3150: train loss: 0.051398\n",
      "1630/3150: train loss: 0.070205\n",
      "1631/3150: train loss: 0.093990\n",
      "1632/3150: train loss: 0.044544\n",
      "1633/3150: train loss: 0.072564\n",
      "1634/3150: train loss: 0.031796\n",
      "1635/3150: train loss: 0.040872\n",
      "1636/3150: train loss: 0.036414\n",
      "1637/3150: train loss: 0.035478\n",
      "1638/3150: train loss: 0.051688\n",
      "1639/3150: train loss: 0.027369\n",
      "1640/3150: train loss: 0.026118\n",
      "1641/3150: train loss: 0.047948\n",
      "1642/3150: train loss: 0.114503\n",
      "1643/3150: train loss: 0.031681\n",
      "1644/3150: train loss: 0.082803\n",
      "1645/3150: train loss: 0.021118\n",
      "1646/3150: train loss: 0.090519\n",
      "1647/3150: train loss: 0.037300\n",
      "1648/3150: train loss: 0.070494\n",
      "1649/3150: train loss: 0.024102\n",
      "1650/3150: train loss: 0.036891\n",
      "1651/3150: train loss: 0.052551\n",
      "1652/3150: train loss: 0.029449\n",
      "1653/3150: train loss: 0.078879\n",
      "1654/3150: train loss: 0.089485\n",
      "1655/3150: train loss: 0.043174\n",
      "1656/3150: train loss: 0.035622\n",
      "1657/3150: train loss: 0.053836\n",
      "1658/3150: train loss: 0.066655\n",
      "1659/3150: train loss: 0.077990\n",
      "1660/3150: train loss: 0.069821\n",
      "1661/3150: train loss: 0.031079\n",
      "1662/3150: train loss: 0.055380\n",
      "1663/3150: train loss: 0.040133\n",
      "1664/3150: train loss: 0.057260\n",
      "1665/3150: train loss: 0.031102\n",
      "1666/3150: train loss: 0.041256\n",
      "1667/3150: train loss: 0.016329\n",
      "1668/3150: train loss: 0.056065\n",
      "1669/3150: train loss: 0.040355\n",
      "1670/3150: train loss: 0.036683\n",
      "1671/3150: train loss: 0.031820\n",
      "1672/3150: train loss: 0.021232\n",
      "1673/3150: train loss: 0.032371\n",
      "1674/3150: train loss: 0.047054\n",
      "1675/3150: train loss: 0.031955\n",
      "1676/3150: train loss: 0.025642\n",
      "1677/3150: train loss: 0.043577\n",
      "1678/3150: train loss: 0.023507\n",
      "1679/3150: train loss: 0.044828\n",
      "1680/3150: train loss: 0.048668\n",
      "1681/3150: train loss: 0.028018\n",
      "1682/3150: train loss: 0.127782\n",
      "1683/3150: train loss: 0.021197\n",
      "1684/3150: train loss: 0.061729\n",
      "1685/3150: train loss: 0.031510\n",
      "1686/3150: train loss: 0.038730\n",
      "1687/3150: train loss: 0.059114\n",
      "1688/3150: train loss: 0.038747\n",
      "1689/3150: train loss: 0.034862\n",
      "1690/3150: train loss: 0.029960\n",
      "1691/3150: train loss: 0.105007\n",
      "1692/3150: train loss: 0.052129\n",
      "1693/3150: train loss: 0.016316\n",
      "1694/3150: train loss: 0.055507\n",
      "1695/3150: train loss: 0.062221\n",
      "1696/3150: train loss: 0.078128\n",
      "1697/3150: train loss: 0.049381\n",
      "1698/3150: train loss: 0.072254\n",
      "1699/3150: train loss: 0.023956\n",
      "1700/3150: train loss: 0.058435\n",
      "1701/3150: train loss: 0.023531\n",
      "1702/3150: train loss: 0.031661\n",
      "1703/3150: train loss: 0.097774\n",
      "1704/3150: train loss: 0.037149\n",
      "1705/3150: train loss: 0.109129\n",
      "1706/3150: train loss: 0.052971\n",
      "1707/3150: train loss: 0.052654\n",
      "1708/3150: train loss: 0.029800\n",
      "1709/3150: train loss: 0.014064\n",
      "1710/3150: train loss: 0.020797\n",
      "1711/3150: train loss: 0.046057\n",
      "1712/3150: train loss: 0.062287\n",
      "1713/3150: train loss: 0.028443\n",
      "1714/3150: train loss: 0.019897\n",
      "1715/3150: train loss: 0.022066\n",
      "1716/3150: train loss: 0.033246\n",
      "1717/3150: train loss: 0.089319\n",
      "1718/3150: train loss: 0.033237\n",
      "1719/3150: train loss: 0.042327\n",
      "1720/3150: train loss: 0.067750\n",
      "1721/3150: train loss: 0.066982\n",
      "1722/3150: train loss: 0.023423\n",
      "1723/3150: train loss: 0.034709\n",
      "1724/3150: train loss: 0.060013\n",
      "1725/3150: train loss: 0.023311\n",
      "1726/3150: train loss: 0.117001\n",
      "1727/3150: train loss: 0.168029\n",
      "1728/3150: train loss: 0.018001\n",
      "1729/3150: train loss: 0.021591\n",
      "1730/3150: train loss: 0.049665\n",
      "1731/3150: train loss: 0.049379\n",
      "1732/3150: train loss: 0.023654\n",
      "1733/3150: train loss: 0.098196\n",
      "1734/3150: train loss: 0.064886\n",
      "1735/3150: train loss: 0.034176\n",
      "1736/3150: train loss: 0.051823\n",
      "1737/3150: train loss: 0.034826\n",
      "1738/3150: train loss: 0.058834\n",
      "1739/3150: train loss: 0.070716\n",
      "1740/3150: train loss: 0.015596\n",
      "1741/3150: train loss: 0.052084\n",
      "1742/3150: train loss: 0.022408\n",
      "1743/3150: train loss: 0.038932\n",
      "1744/3150: train loss: 0.052350\n",
      "1745/3150: train loss: 0.035286\n",
      "1746/3150: train loss: 0.065385\n",
      "1747/3150: train loss: 0.035677\n",
      "1748/3150: train loss: 0.051256\n",
      "1749/3150: train loss: 0.058107\n",
      "1750/3150: train loss: 0.065925\n",
      "1751/3150: train loss: 0.085359\n",
      "1752/3150: train loss: 0.066823\n",
      "1753/3150: train loss: 0.032033\n",
      "1754/3150: train loss: 0.039472\n",
      "1755/3150: train loss: 0.034571\n",
      "1756/3150: train loss: 0.044046\n",
      "1757/3150: train loss: 0.060046\n",
      "1758/3150: train loss: 0.064627\n",
      "1759/3150: train loss: 0.055454\n",
      "1760/3150: train loss: 0.038632\n",
      "1761/3150: train loss: 0.129229\n",
      "1762/3150: train loss: 0.037099\n",
      "1763/3150: train loss: 0.038065\n",
      "1764/3150: train loss: 0.067617\n",
      "1765/3150: train loss: 0.026076\n",
      "1766/3150: train loss: 0.045971\n",
      "1767/3150: train loss: 0.049946\n",
      "1768/3150: train loss: 0.035988\n",
      "1769/3150: train loss: 0.019468\n",
      "1770/3150: train loss: 0.017113\n",
      "1771/3150: train loss: 0.025790\n",
      "1772/3150: train loss: 0.032173\n",
      "1773/3150: train loss: 0.036312\n",
      "1774/3150: train loss: 0.058717\n",
      "1775/3150: train loss: 0.075111\n",
      "1776/3150: train loss: 0.041900\n",
      "1777/3150: train loss: 0.024847\n",
      "1778/3150: train loss: 0.043609\n",
      "1779/3150: train loss: 0.032650\n",
      "1780/3150: train loss: 0.027707\n",
      "1781/3150: train loss: 0.039611\n",
      "1782/3150: train loss: 0.081663\n",
      "1783/3150: train loss: 0.042960\n",
      "1784/3150: train loss: 0.017371\n",
      "1785/3150: train loss: 0.032497\n",
      "1786/3150: train loss: 0.094060\n",
      "1787/3150: train loss: 0.026953\n",
      "1788/3150: train loss: 0.089896\n",
      "1789/3150: train loss: 0.021315\n",
      "1790/3150: train loss: 0.030143\n",
      "1791/3150: train loss: 0.019317\n",
      "1792/3150: train loss: 0.029168\n",
      "1793/3150: train loss: 0.017324\n",
      "1794/3150: train loss: 0.031786\n",
      "1795/3150: train loss: 0.031307\n",
      "1796/3150: train loss: 0.038551\n",
      "1797/3150: train loss: 0.023302\n",
      "1798/3150: train loss: 0.073331\n",
      "1799/3150: train loss: 0.020137\n",
      "1800/3150: train loss: 0.045431\n",
      "1801/3150: train loss: 0.067090\n",
      "1802/3150: train loss: 0.019193\n",
      "1803/3150: train loss: 0.067029\n",
      "1804/3150: train loss: 0.060985\n",
      "1805/3150: train loss: 0.035125\n",
      "1806/3150: train loss: 0.093857\n",
      "1807/3150: train loss: 0.060783\n",
      "1808/3150: train loss: 0.032573\n",
      "1809/3150: train loss: 0.045511\n",
      "1810/3150: train loss: 0.030039\n",
      "1811/3150: train loss: 0.041553\n",
      "1812/3150: train loss: 0.079818\n",
      "1813/3150: train loss: 0.038207\n",
      "1814/3150: train loss: 0.036226\n",
      "1815/3150: train loss: 0.029569\n",
      "1816/3150: train loss: 0.076448\n",
      "1817/3150: train loss: 0.043957\n",
      "1818/3150: train loss: 0.055191\n",
      "1819/3150: train loss: 0.075370\n",
      "1820/3150: train loss: 0.051490\n",
      "1821/3150: train loss: 0.024084\n",
      "1822/3150: train loss: 0.074160\n",
      "1823/3150: train loss: 0.038466\n",
      "1824/3150: train loss: 0.023448\n",
      "1825/3150: train loss: 0.028251\n",
      "1826/3150: train loss: 0.068769\n",
      "1827/3150: train loss: 0.035030\n",
      "1828/3150: train loss: 0.099140\n",
      "1829/3150: train loss: 0.019547\n",
      "1830/3150: train loss: 0.050309\n",
      "1831/3150: train loss: 0.024249\n",
      "1832/3150: train loss: 0.020116\n",
      "1833/3150: train loss: 0.028003\n",
      "1834/3150: train loss: 0.033886\n",
      "1835/3150: train loss: 0.024666\n",
      "1836/3150: train loss: 0.050215\n",
      "1837/3150: train loss: 0.012980\n",
      "1838/3150: train loss: 0.061985\n",
      "1839/3150: train loss: 0.097962\n",
      "1840/3150: train loss: 0.017507\n",
      "1841/3150: train loss: 0.088651\n",
      "1842/3150: train loss: 0.046675\n",
      "1843/3150: train loss: 0.031731\n",
      "1844/3150: train loss: 0.025883\n",
      "1845/3150: train loss: 0.025820\n",
      "1846/3150: train loss: 0.030117\n",
      "1847/3150: train loss: 0.040037\n",
      "1848/3150: train loss: 0.040327\n",
      "1849/3150: train loss: 0.024194\n",
      "1850/3150: train loss: 0.020160\n",
      "1851/3150: train loss: 0.038194\n",
      "1852/3150: train loss: 0.038554\n",
      "1853/3150: train loss: 0.021258\n",
      "1854/3150: train loss: 0.067036\n",
      "1855/3150: train loss: 0.032752\n",
      "1856/3150: train loss: 0.055904\n",
      "1857/3150: train loss: 0.046301\n",
      "1858/3150: train loss: 0.023126\n",
      "1859/3150: train loss: 0.023267\n",
      "1860/3150: train loss: 0.027490\n",
      "1861/3150: train loss: 0.044963\n",
      "1862/3150: train loss: 0.073514\n",
      "1863/3150: train loss: 0.053933\n",
      "1864/3150: train loss: 0.043003\n",
      "1865/3150: train loss: 0.017158\n",
      "1866/3150: train loss: 0.022723\n",
      "1867/3150: train loss: 0.030191\n",
      "1868/3150: train loss: 0.041113\n",
      "1869/3150: train loss: 0.030891\n",
      "1870/3150: train loss: 0.045606\n",
      "1871/3150: train loss: 0.017393\n",
      "1872/3150: train loss: 0.010407\n",
      "1873/3150: train loss: 0.046842\n",
      "1874/3150: train loss: 0.055567\n",
      "1875/3150: train loss: 0.033402\n",
      "1876/3150: train loss: 0.062019\n",
      "1877/3150: train loss: 0.045729\n",
      "1878/3150: train loss: 0.038460\n",
      "1879/3150: train loss: 0.017907\n",
      "1880/3150: train loss: 0.048985\n",
      "1881/3150: train loss: 0.027762\n",
      "1882/3150: train loss: 0.026995\n",
      "1883/3150: train loss: 0.018653\n",
      "1884/3150: train loss: 0.032748\n",
      "1885/3150: train loss: 0.066398\n",
      "1886/3150: train loss: 0.022582\n",
      "1887/3150: train loss: 0.039471\n",
      "1888/3150: train loss: 0.043849\n",
      "1889/3150: train loss: 0.030675\n",
      "(0.9819365079365079, 0.9757142857142858)\n",
      "1890/3150: train loss: 0.047723\n",
      "1891/3150: train loss: 0.033650\n",
      "1892/3150: train loss: 0.027379\n",
      "1893/3150: train loss: 0.038422\n",
      "1894/3150: train loss: 0.025488\n",
      "1895/3150: train loss: 0.031625\n",
      "1896/3150: train loss: 0.022559\n",
      "1897/3150: train loss: 0.024416\n",
      "1898/3150: train loss: 0.052344\n",
      "1899/3150: train loss: 0.025743\n",
      "1900/3150: train loss: 0.019027\n",
      "1901/3150: train loss: 0.027577\n",
      "1902/3150: train loss: 0.040221\n",
      "1903/3150: train loss: 0.039268\n",
      "1904/3150: train loss: 0.027405\n",
      "1905/3150: train loss: 0.017761\n",
      "1906/3150: train loss: 0.061125\n",
      "1907/3150: train loss: 0.024845\n",
      "1908/3150: train loss: 0.019342\n",
      "1909/3150: train loss: 0.044122\n",
      "1910/3150: train loss: 0.026713\n",
      "1911/3150: train loss: 0.047937\n",
      "1912/3150: train loss: 0.120730\n",
      "1913/3150: train loss: 0.032903\n",
      "1914/3150: train loss: 0.012520\n",
      "1915/3150: train loss: 0.036546\n",
      "1916/3150: train loss: 0.052699\n",
      "1917/3150: train loss: 0.032329\n",
      "1918/3150: train loss: 0.040635\n",
      "1919/3150: train loss: 0.056286\n",
      "1920/3150: train loss: 0.120914\n",
      "1921/3150: train loss: 0.055334\n",
      "1922/3150: train loss: 0.023096\n",
      "1923/3150: train loss: 0.032801\n",
      "1924/3150: train loss: 0.069123\n",
      "1925/3150: train loss: 0.019447\n",
      "1926/3150: train loss: 0.028686\n",
      "1927/3150: train loss: 0.055869\n",
      "1928/3150: train loss: 0.038257\n",
      "1929/3150: train loss: 0.029965\n",
      "1930/3150: train loss: 0.026072\n",
      "1931/3150: train loss: 0.038141\n",
      "1932/3150: train loss: 0.031599\n",
      "1933/3150: train loss: 0.075750\n",
      "1934/3150: train loss: 0.026365\n",
      "1935/3150: train loss: 0.023971\n",
      "1936/3150: train loss: 0.027945\n",
      "1937/3150: train loss: 0.037787\n",
      "1938/3150: train loss: 0.019871\n",
      "1939/3150: train loss: 0.110965\n",
      "1940/3150: train loss: 0.056178\n",
      "1941/3150: train loss: 0.039019\n",
      "1942/3150: train loss: 0.073354\n",
      "1943/3150: train loss: 0.052044\n",
      "1944/3150: train loss: 0.038163\n",
      "1945/3150: train loss: 0.017784\n",
      "1946/3150: train loss: 0.018445\n",
      "1947/3150: train loss: 0.056457\n",
      "1948/3150: train loss: 0.120096\n",
      "1949/3150: train loss: 0.033764\n",
      "1950/3150: train loss: 0.014259\n",
      "1951/3150: train loss: 0.043838\n",
      "1952/3150: train loss: 0.019655\n",
      "1953/3150: train loss: 0.035329\n",
      "1954/3150: train loss: 0.040887\n",
      "1955/3150: train loss: 0.032788\n",
      "1956/3150: train loss: 0.024834\n",
      "1957/3150: train loss: 0.033221\n",
      "1958/3150: train loss: 0.023903\n",
      "1959/3150: train loss: 0.053334\n",
      "1960/3150: train loss: 0.022488\n",
      "1961/3150: train loss: 0.027637\n",
      "1962/3150: train loss: 0.045732\n",
      "1963/3150: train loss: 0.046286\n",
      "1964/3150: train loss: 0.102259\n",
      "1965/3150: train loss: 0.022897\n",
      "1966/3150: train loss: 0.028082\n",
      "1967/3150: train loss: 0.088119\n",
      "1968/3150: train loss: 0.021848\n",
      "1969/3150: train loss: 0.033905\n",
      "1970/3150: train loss: 0.047288\n",
      "1971/3150: train loss: 0.051296\n",
      "1972/3150: train loss: 0.024098\n",
      "1973/3150: train loss: 0.016088\n",
      "1974/3150: train loss: 0.072061\n",
      "1975/3150: train loss: 0.028009\n",
      "1976/3150: train loss: 0.020058\n",
      "1977/3150: train loss: 0.026811\n",
      "1978/3150: train loss: 0.084961\n",
      "1979/3150: train loss: 0.036119\n",
      "1980/3150: train loss: 0.031341\n",
      "1981/3150: train loss: 0.044586\n",
      "1982/3150: train loss: 0.027337\n",
      "1983/3150: train loss: 0.027396\n",
      "1984/3150: train loss: 0.055456\n",
      "1985/3150: train loss: 0.030625\n",
      "1986/3150: train loss: 0.028608\n",
      "1987/3150: train loss: 0.029817\n",
      "1988/3150: train loss: 0.018083\n",
      "1989/3150: train loss: 0.033709\n",
      "1990/3150: train loss: 0.031317\n",
      "1991/3150: train loss: 0.017771\n",
      "1992/3150: train loss: 0.083106\n",
      "1993/3150: train loss: 0.024345\n",
      "1994/3150: train loss: 0.044465\n",
      "1995/3150: train loss: 0.038915\n",
      "1996/3150: train loss: 0.033304\n",
      "1997/3150: train loss: 0.031342\n",
      "1998/3150: train loss: 0.098509\n",
      "1999/3150: train loss: 0.056973\n",
      "2000/3150: train loss: 0.045280\n",
      "2001/3150: train loss: 0.028438\n",
      "2002/3150: train loss: 0.033879\n",
      "2003/3150: train loss: 0.030779\n",
      "2004/3150: train loss: 0.083486\n",
      "2005/3150: train loss: 0.061219\n",
      "2006/3150: train loss: 0.053451\n",
      "2007/3150: train loss: 0.019921\n",
      "2008/3150: train loss: 0.026056\n",
      "2009/3150: train loss: 0.031274\n",
      "2010/3150: train loss: 0.037165\n",
      "2011/3150: train loss: 0.020038\n",
      "2012/3150: train loss: 0.018868\n",
      "2013/3150: train loss: 0.085325\n",
      "2014/3150: train loss: 0.045031\n",
      "2015/3150: train loss: 0.028863\n",
      "2016/3150: train loss: 0.020524\n",
      "2017/3150: train loss: 0.027673\n",
      "2018/3150: train loss: 0.085636\n",
      "2019/3150: train loss: 0.011489\n",
      "2020/3150: train loss: 0.056604\n",
      "2021/3150: train loss: 0.017636\n",
      "2022/3150: train loss: 0.070754\n",
      "2023/3150: train loss: 0.018407\n",
      "2024/3150: train loss: 0.015755\n",
      "2025/3150: train loss: 0.034757\n",
      "2026/3150: train loss: 0.036103\n",
      "2027/3150: train loss: 0.048653\n",
      "2028/3150: train loss: 0.010331\n",
      "2029/3150: train loss: 0.046421\n",
      "2030/3150: train loss: 0.024189\n",
      "2031/3150: train loss: 0.080877\n",
      "2032/3150: train loss: 0.035461\n",
      "2033/3150: train loss: 0.053860\n",
      "2034/3150: train loss: 0.031020\n",
      "2035/3150: train loss: 0.041804\n",
      "2036/3150: train loss: 0.022005\n",
      "2037/3150: train loss: 0.028892\n",
      "2038/3150: train loss: 0.031073\n",
      "2039/3150: train loss: 0.066702\n",
      "2040/3150: train loss: 0.059781\n",
      "2041/3150: train loss: 0.035906\n",
      "2042/3150: train loss: 0.082944\n",
      "2043/3150: train loss: 0.082922\n",
      "2044/3150: train loss: 0.051161\n",
      "2045/3150: train loss: 0.067122\n",
      "2046/3150: train loss: 0.058101\n",
      "2047/3150: train loss: 0.056059\n",
      "2048/3150: train loss: 0.023010\n",
      "2049/3150: train loss: 0.049183\n",
      "2050/3150: train loss: 0.041947\n",
      "2051/3150: train loss: 0.025968\n",
      "2052/3150: train loss: 0.033597\n",
      "2053/3150: train loss: 0.015918\n",
      "2054/3150: train loss: 0.049129\n",
      "2055/3150: train loss: 0.021140\n",
      "2056/3150: train loss: 0.028134\n",
      "2057/3150: train loss: 0.017100\n",
      "2058/3150: train loss: 0.021678\n",
      "2059/3150: train loss: 0.032709\n",
      "2060/3150: train loss: 0.043161\n",
      "2061/3150: train loss: 0.013461\n",
      "2062/3150: train loss: 0.089318\n",
      "2063/3150: train loss: 0.031232\n",
      "2064/3150: train loss: 0.017487\n",
      "2065/3150: train loss: 0.035265\n",
      "2066/3150: train loss: 0.051146\n",
      "2067/3150: train loss: 0.021638\n",
      "2068/3150: train loss: 0.032443\n",
      "2069/3150: train loss: 0.024583\n",
      "2070/3150: train loss: 0.040439\n",
      "2071/3150: train loss: 0.026334\n",
      "2072/3150: train loss: 0.035503\n",
      "2073/3150: train loss: 0.021189\n",
      "2074/3150: train loss: 0.027060\n",
      "2075/3150: train loss: 0.022111\n",
      "2076/3150: train loss: 0.024437\n",
      "2077/3150: train loss: 0.028994\n",
      "2078/3150: train loss: 0.024747\n",
      "2079/3150: train loss: 0.041194\n",
      "2080/3150: train loss: 0.017558\n",
      "2081/3150: train loss: 0.018327\n",
      "2082/3150: train loss: 0.040126\n",
      "2083/3150: train loss: 0.020611\n",
      "2084/3150: train loss: 0.034234\n",
      "2085/3150: train loss: 0.021181\n",
      "2086/3150: train loss: 0.020876\n",
      "2087/3150: train loss: 0.027770\n",
      "2088/3150: train loss: 0.021033\n",
      "2089/3150: train loss: 0.183445\n",
      "2090/3150: train loss: 0.022204\n",
      "2091/3150: train loss: 0.061982\n",
      "2092/3150: train loss: 0.011487\n",
      "2093/3150: train loss: 0.022337\n",
      "2094/3150: train loss: 0.024415\n",
      "2095/3150: train loss: 0.058956\n",
      "2096/3150: train loss: 0.028724\n",
      "2097/3150: train loss: 0.022875\n",
      "2098/3150: train loss: 0.021200\n",
      "2099/3150: train loss: 0.069026\n",
      "2100/3150: train loss: 0.018158\n",
      "2101/3150: train loss: 0.024202\n",
      "2102/3150: train loss: 0.007940\n",
      "2103/3150: train loss: 0.046380\n",
      "2104/3150: train loss: 0.039103\n",
      "2105/3150: train loss: 0.028026\n",
      "2106/3150: train loss: 0.028399\n",
      "2107/3150: train loss: 0.046139\n",
      "2108/3150: train loss: 0.021793\n",
      "2109/3150: train loss: 0.055504\n",
      "2110/3150: train loss: 0.029985\n",
      "2111/3150: train loss: 0.044202\n",
      "2112/3150: train loss: 0.089859\n",
      "2113/3150: train loss: 0.032824\n",
      "2114/3150: train loss: 0.040825\n",
      "2115/3150: train loss: 0.036656\n",
      "2116/3150: train loss: 0.050528\n",
      "2117/3150: train loss: 0.140357\n",
      "2118/3150: train loss: 0.016114\n",
      "2119/3150: train loss: 0.046337\n",
      "2120/3150: train loss: 0.042284\n",
      "2121/3150: train loss: 0.046145\n",
      "2122/3150: train loss: 0.022416\n",
      "2123/3150: train loss: 0.058657\n",
      "2124/3150: train loss: 0.031963\n",
      "2125/3150: train loss: 0.057287\n",
      "2126/3150: train loss: 0.016200\n",
      "2127/3150: train loss: 0.034168\n",
      "2128/3150: train loss: 0.034389\n",
      "2129/3150: train loss: 0.041584\n",
      "2130/3150: train loss: 0.057174\n",
      "2131/3150: train loss: 0.049155\n",
      "2132/3150: train loss: 0.025443\n",
      "2133/3150: train loss: 0.026511\n",
      "2134/3150: train loss: 0.045055\n",
      "2135/3150: train loss: 0.043274\n",
      "2136/3150: train loss: 0.060756\n",
      "2137/3150: train loss: 0.049377\n",
      "2138/3150: train loss: 0.041705\n",
      "2139/3150: train loss: 0.013721\n",
      "2140/3150: train loss: 0.023956\n",
      "2141/3150: train loss: 0.039654\n",
      "2142/3150: train loss: 0.012984\n",
      "2143/3150: train loss: 0.029284\n",
      "2144/3150: train loss: 0.132228\n",
      "2145/3150: train loss: 0.024753\n",
      "2146/3150: train loss: 0.038654\n",
      "2147/3150: train loss: 0.056057\n",
      "2148/3150: train loss: 0.022535\n",
      "2149/3150: train loss: 0.051381\n",
      "2150/3150: train loss: 0.076051\n",
      "2151/3150: train loss: 0.026713\n",
      "2152/3150: train loss: 0.089795\n",
      "2153/3150: train loss: 0.037711\n",
      "2154/3150: train loss: 0.036270\n",
      "2155/3150: train loss: 0.057636\n",
      "2156/3150: train loss: 0.019032\n",
      "2157/3150: train loss: 0.020162\n",
      "2158/3150: train loss: 0.045205\n",
      "2159/3150: train loss: 0.029043\n",
      "2160/3150: train loss: 0.038722\n",
      "2161/3150: train loss: 0.028705\n",
      "2162/3150: train loss: 0.040186\n",
      "2163/3150: train loss: 0.067979\n",
      "2164/3150: train loss: 0.025016\n",
      "2165/3150: train loss: 0.066780\n",
      "2166/3150: train loss: 0.050322\n",
      "2167/3150: train loss: 0.054433\n",
      "2168/3150: train loss: 0.051917\n",
      "2169/3150: train loss: 0.082655\n",
      "2170/3150: train loss: 0.035638\n",
      "2171/3150: train loss: 0.019069\n",
      "2172/3150: train loss: 0.029381\n",
      "2173/3150: train loss: 0.029471\n",
      "2174/3150: train loss: 0.024476\n",
      "2175/3150: train loss: 0.042496\n",
      "2176/3150: train loss: 0.034104\n",
      "2177/3150: train loss: 0.059757\n",
      "2178/3150: train loss: 0.040031\n",
      "2179/3150: train loss: 0.077108\n",
      "2180/3150: train loss: 0.015387\n",
      "2181/3150: train loss: 0.028231\n",
      "2182/3150: train loss: 0.060652\n",
      "2183/3150: train loss: 0.026107\n",
      "2184/3150: train loss: 0.061381\n",
      "2185/3150: train loss: 0.034024\n",
      "2186/3150: train loss: 0.025168\n",
      "2187/3150: train loss: 0.038161\n",
      "2188/3150: train loss: 0.019218\n",
      "2189/3150: train loss: 0.054483\n",
      "2190/3150: train loss: 0.060145\n",
      "2191/3150: train loss: 0.031529\n",
      "2192/3150: train loss: 0.046530\n",
      "2193/3150: train loss: 0.039063\n",
      "2194/3150: train loss: 0.029622\n",
      "2195/3150: train loss: 0.015502\n",
      "2196/3150: train loss: 0.028257\n",
      "2197/3150: train loss: 0.031273\n",
      "2198/3150: train loss: 0.025515\n",
      "2199/3150: train loss: 0.016658\n",
      "2200/3150: train loss: 0.033833\n",
      "2201/3150: train loss: 0.015925\n",
      "2202/3150: train loss: 0.119128\n",
      "2203/3150: train loss: 0.021076\n",
      "2204/3150: train loss: 0.065816\n",
      "(0.9835873015873016, 0.9780952380952381)\n",
      "2205/3150: train loss: 0.015925\n",
      "2206/3150: train loss: 0.049299\n",
      "2207/3150: train loss: 0.026910\n",
      "2208/3150: train loss: 0.066074\n",
      "2209/3150: train loss: 0.034976\n",
      "2210/3150: train loss: 0.047298\n",
      "2211/3150: train loss: 0.022817\n",
      "2212/3150: train loss: 0.056441\n",
      "2213/3150: train loss: 0.044893\n",
      "2214/3150: train loss: 0.034046\n",
      "2215/3150: train loss: 0.040298\n",
      "2216/3150: train loss: 0.080914\n",
      "2217/3150: train loss: 0.015023\n",
      "2218/3150: train loss: 0.030285\n",
      "2219/3150: train loss: 0.019592\n",
      "2220/3150: train loss: 0.087025\n",
      "2221/3150: train loss: 0.033737\n",
      "2222/3150: train loss: 0.029164\n",
      "2223/3150: train loss: 0.030748\n",
      "2224/3150: train loss: 0.016519\n",
      "2225/3150: train loss: 0.016473\n",
      "2226/3150: train loss: 0.051453\n",
      "2227/3150: train loss: 0.032438\n",
      "2228/3150: train loss: 0.096606\n",
      "2229/3150: train loss: 0.016874\n",
      "2230/3150: train loss: 0.037167\n",
      "2231/3150: train loss: 0.070679\n",
      "2232/3150: train loss: 0.019670\n",
      "2233/3150: train loss: 0.057433\n",
      "2234/3150: train loss: 0.052359\n",
      "2235/3150: train loss: 0.019815\n",
      "2236/3150: train loss: 0.043748\n",
      "2237/3150: train loss: 0.042977\n",
      "2238/3150: train loss: 0.058691\n",
      "2239/3150: train loss: 0.035566\n",
      "2240/3150: train loss: 0.016603\n",
      "2241/3150: train loss: 0.024279\n",
      "2242/3150: train loss: 0.025762\n",
      "2243/3150: train loss: 0.029271\n",
      "2244/3150: train loss: 0.031555\n",
      "2245/3150: train loss: 0.040072\n",
      "2246/3150: train loss: 0.042109\n",
      "2247/3150: train loss: 0.029703\n",
      "2248/3150: train loss: 0.088835\n",
      "2249/3150: train loss: 0.046045\n",
      "2250/3150: train loss: 0.060925\n",
      "2251/3150: train loss: 0.029961\n",
      "2252/3150: train loss: 0.021667\n",
      "2253/3150: train loss: 0.029490\n",
      "2254/3150: train loss: 0.051648\n",
      "2255/3150: train loss: 0.033149\n",
      "2256/3150: train loss: 0.037960\n",
      "2257/3150: train loss: 0.026390\n",
      "2258/3150: train loss: 0.075212\n",
      "2259/3150: train loss: 0.044825\n",
      "2260/3150: train loss: 0.044220\n",
      "2261/3150: train loss: 0.016140\n",
      "2262/3150: train loss: 0.053370\n",
      "2263/3150: train loss: 0.023225\n",
      "2264/3150: train loss: 0.053166\n",
      "2265/3150: train loss: 0.045508\n",
      "2266/3150: train loss: 0.076066\n",
      "2267/3150: train loss: 0.050635\n",
      "2268/3150: train loss: 0.009881\n",
      "2269/3150: train loss: 0.053040\n",
      "2270/3150: train loss: 0.013379\n",
      "2271/3150: train loss: 0.054717\n",
      "2272/3150: train loss: 0.045720\n",
      "2273/3150: train loss: 0.055305\n",
      "2274/3150: train loss: 0.028986\n",
      "2275/3150: train loss: 0.028991\n",
      "2276/3150: train loss: 0.037563\n",
      "2277/3150: train loss: 0.027584\n",
      "2278/3150: train loss: 0.025099\n",
      "2279/3150: train loss: 0.031519\n",
      "2280/3150: train loss: 0.061727\n",
      "2281/3150: train loss: 0.053570\n",
      "2282/3150: train loss: 0.028605\n",
      "2283/3150: train loss: 0.026697\n",
      "2284/3150: train loss: 0.033742\n",
      "2285/3150: train loss: 0.041735\n",
      "2286/3150: train loss: 0.029314\n",
      "2287/3150: train loss: 0.037757\n",
      "2288/3150: train loss: 0.026755\n",
      "2289/3150: train loss: 0.016407\n",
      "2290/3150: train loss: 0.029871\n",
      "2291/3150: train loss: 0.033553\n",
      "2292/3150: train loss: 0.037593\n",
      "2293/3150: train loss: 0.027396\n",
      "2294/3150: train loss: 0.053893\n",
      "2295/3150: train loss: 0.047276\n",
      "2296/3150: train loss: 0.038877\n",
      "2297/3150: train loss: 0.058342\n",
      "2298/3150: train loss: 0.016359\n",
      "2299/3150: train loss: 0.022339\n",
      "2300/3150: train loss: 0.066025\n",
      "2301/3150: train loss: 0.027827\n",
      "2302/3150: train loss: 0.014461\n",
      "2303/3150: train loss: 0.033894\n",
      "2304/3150: train loss: 0.046123\n",
      "2305/3150: train loss: 0.031556\n",
      "2306/3150: train loss: 0.078490\n",
      "2307/3150: train loss: 0.015766\n",
      "2308/3150: train loss: 0.019420\n",
      "2309/3150: train loss: 0.027308\n",
      "2310/3150: train loss: 0.019910\n",
      "2311/3150: train loss: 0.020349\n",
      "2312/3150: train loss: 0.026638\n",
      "2313/3150: train loss: 0.049258\n",
      "2314/3150: train loss: 0.030041\n",
      "2315/3150: train loss: 0.052814\n",
      "2316/3150: train loss: 0.030620\n",
      "2317/3150: train loss: 0.066809\n",
      "2318/3150: train loss: 0.018548\n",
      "2319/3150: train loss: 0.047835\n",
      "2320/3150: train loss: 0.025092\n",
      "2321/3150: train loss: 0.018864\n",
      "2322/3150: train loss: 0.022962\n",
      "2323/3150: train loss: 0.030518\n",
      "2324/3150: train loss: 0.018470\n",
      "2325/3150: train loss: 0.024800\n",
      "2326/3150: train loss: 0.010691\n",
      "2327/3150: train loss: 0.023700\n",
      "2328/3150: train loss: 0.053037\n",
      "2329/3150: train loss: 0.045280\n",
      "2330/3150: train loss: 0.029487\n",
      "2331/3150: train loss: 0.066138\n",
      "2332/3150: train loss: 0.020762\n",
      "2333/3150: train loss: 0.018605\n",
      "2334/3150: train loss: 0.027016\n",
      "2335/3150: train loss: 0.013704\n",
      "2336/3150: train loss: 0.026757\n",
      "2337/3150: train loss: 0.011706\n",
      "2338/3150: train loss: 0.025325\n",
      "2339/3150: train loss: 0.022752\n",
      "2340/3150: train loss: 0.061223\n",
      "2341/3150: train loss: 0.048520\n",
      "2342/3150: train loss: 0.023537\n",
      "2343/3150: train loss: 0.025271\n",
      "2344/3150: train loss: 0.051668\n",
      "2345/3150: train loss: 0.039154\n",
      "2346/3150: train loss: 0.059236\n",
      "2347/3150: train loss: 0.043112\n",
      "2348/3150: train loss: 0.033124\n",
      "2349/3150: train loss: 0.032568\n",
      "2350/3150: train loss: 0.033715\n",
      "2351/3150: train loss: 0.020932\n",
      "2352/3150: train loss: 0.020034\n",
      "2353/3150: train loss: 0.037399\n",
      "2354/3150: train loss: 0.027621\n",
      "2355/3150: train loss: 0.032172\n",
      "2356/3150: train loss: 0.020858\n",
      "2357/3150: train loss: 0.018276\n",
      "2358/3150: train loss: 0.019394\n",
      "2359/3150: train loss: 0.016147\n",
      "2360/3150: train loss: 0.072338\n",
      "2361/3150: train loss: 0.022119\n",
      "2362/3150: train loss: 0.017199\n",
      "2363/3150: train loss: 0.017062\n",
      "2364/3150: train loss: 0.027051\n",
      "2365/3150: train loss: 0.015634\n",
      "2366/3150: train loss: 0.029284\n",
      "2367/3150: train loss: 0.018759\n",
      "2368/3150: train loss: 0.033184\n",
      "2369/3150: train loss: 0.048650\n",
      "2370/3150: train loss: 0.029315\n",
      "2371/3150: train loss: 0.057446\n",
      "2372/3150: train loss: 0.036123\n",
      "2373/3150: train loss: 0.049517\n",
      "2374/3150: train loss: 0.019664\n",
      "2375/3150: train loss: 0.015184\n",
      "2376/3150: train loss: 0.043625\n",
      "2377/3150: train loss: 0.078539\n",
      "2378/3150: train loss: 0.018043\n",
      "2379/3150: train loss: 0.018199\n",
      "2380/3150: train loss: 0.011822\n",
      "2381/3150: train loss: 0.045757\n",
      "2382/3150: train loss: 0.020001\n",
      "2383/3150: train loss: 0.047572\n",
      "2384/3150: train loss: 0.031384\n",
      "2385/3150: train loss: 0.035712\n",
      "2386/3150: train loss: 0.032341\n",
      "2387/3150: train loss: 0.026799\n",
      "2388/3150: train loss: 0.034330\n",
      "2389/3150: train loss: 0.022920\n",
      "2390/3150: train loss: 0.011587\n",
      "2391/3150: train loss: 0.057364\n",
      "2392/3150: train loss: 0.023850\n",
      "2393/3150: train loss: 0.035126\n",
      "2394/3150: train loss: 0.030175\n",
      "2395/3150: train loss: 0.032448\n",
      "2396/3150: train loss: 0.013144\n",
      "2397/3150: train loss: 0.064774\n",
      "2398/3150: train loss: 0.019301\n",
      "2399/3150: train loss: 0.046716\n",
      "2400/3150: train loss: 0.050559\n",
      "2401/3150: train loss: 0.029496\n",
      "2402/3150: train loss: 0.024476\n",
      "2403/3150: train loss: 0.055100\n",
      "2404/3150: train loss: 0.032336\n",
      "2405/3150: train loss: 0.029469\n",
      "2406/3150: train loss: 0.024175\n",
      "2407/3150: train loss: 0.031044\n",
      "2408/3150: train loss: 0.044424\n",
      "2409/3150: train loss: 0.031307\n",
      "2410/3150: train loss: 0.055507\n",
      "2411/3150: train loss: 0.043371\n",
      "2412/3150: train loss: 0.022993\n",
      "2413/3150: train loss: 0.021220\n",
      "2414/3150: train loss: 0.051630\n",
      "2415/3150: train loss: 0.038589\n",
      "2416/3150: train loss: 0.050713\n",
      "2417/3150: train loss: 0.031869\n",
      "2418/3150: train loss: 0.029770\n",
      "2419/3150: train loss: 0.019231\n",
      "2420/3150: train loss: 0.036781\n",
      "2421/3150: train loss: 0.029564\n",
      "2422/3150: train loss: 0.034697\n",
      "2423/3150: train loss: 0.027643\n",
      "2424/3150: train loss: 0.099322\n",
      "2425/3150: train loss: 0.022053\n",
      "2426/3150: train loss: 0.059770\n",
      "2427/3150: train loss: 0.028596\n",
      "2428/3150: train loss: 0.039040\n",
      "2429/3150: train loss: 0.034383\n",
      "2430/3150: train loss: 0.012750\n",
      "2431/3150: train loss: 0.041544\n",
      "2432/3150: train loss: 0.069899\n",
      "2433/3150: train loss: 0.030407\n",
      "2434/3150: train loss: 0.078180\n",
      "2435/3150: train loss: 0.053461\n",
      "2436/3150: train loss: 0.063627\n",
      "2437/3150: train loss: 0.012935\n",
      "2438/3150: train loss: 0.019779\n",
      "2439/3150: train loss: 0.010967\n",
      "2440/3150: train loss: 0.022285\n",
      "2441/3150: train loss: 0.040534\n",
      "2442/3150: train loss: 0.033298\n",
      "2443/3150: train loss: 0.029946\n",
      "2444/3150: train loss: 0.063604\n",
      "2445/3150: train loss: 0.020219\n",
      "2446/3150: train loss: 0.028605\n",
      "2447/3150: train loss: 0.025148\n",
      "2448/3150: train loss: 0.029393\n",
      "2449/3150: train loss: 0.051318\n",
      "2450/3150: train loss: 0.015912\n",
      "2451/3150: train loss: 0.033572\n",
      "2452/3150: train loss: 0.023430\n",
      "2453/3150: train loss: 0.036471\n",
      "2454/3150: train loss: 0.057208\n",
      "2455/3150: train loss: 0.015056\n",
      "2456/3150: train loss: 0.025286\n",
      "2457/3150: train loss: 0.024929\n",
      "2458/3150: train loss: 0.100844\n",
      "2459/3150: train loss: 0.020778\n",
      "2460/3150: train loss: 0.018553\n",
      "2461/3150: train loss: 0.026739\n",
      "2462/3150: train loss: 0.040290\n",
      "2463/3150: train loss: 0.026033\n",
      "2464/3150: train loss: 0.051390\n",
      "2465/3150: train loss: 0.090991\n",
      "2466/3150: train loss: 0.033934\n",
      "2467/3150: train loss: 0.025018\n",
      "2468/3150: train loss: 0.031900\n",
      "2469/3150: train loss: 0.016143\n",
      "2470/3150: train loss: 0.011891\n",
      "2471/3150: train loss: 0.026564\n",
      "2472/3150: train loss: 0.026958\n",
      "2473/3150: train loss: 0.009597\n",
      "2474/3150: train loss: 0.008535\n",
      "2475/3150: train loss: 0.031245\n",
      "2476/3150: train loss: 0.039438\n",
      "2477/3150: train loss: 0.044114\n",
      "2478/3150: train loss: 0.015090\n",
      "2479/3150: train loss: 0.044628\n",
      "2480/3150: train loss: 0.025606\n",
      "2481/3150: train loss: 0.017208\n",
      "2482/3150: train loss: 0.032403\n",
      "2483/3150: train loss: 0.018934\n",
      "2484/3150: train loss: 0.031116\n",
      "2485/3150: train loss: 0.021037\n",
      "2486/3150: train loss: 0.029472\n",
      "2487/3150: train loss: 0.016289\n",
      "2488/3150: train loss: 0.013536\n",
      "2489/3150: train loss: 0.016989\n",
      "2490/3150: train loss: 0.033421\n",
      "2491/3150: train loss: 0.013662\n",
      "2492/3150: train loss: 0.025449\n",
      "2493/3150: train loss: 0.033801\n",
      "2494/3150: train loss: 0.011533\n",
      "2495/3150: train loss: 0.023668\n",
      "2496/3150: train loss: 0.028960\n",
      "2497/3150: train loss: 0.027826\n",
      "2498/3150: train loss: 0.016256\n",
      "2499/3150: train loss: 0.025798\n",
      "2500/3150: train loss: 0.024926\n",
      "2501/3150: train loss: 0.025773\n",
      "2502/3150: train loss: 0.021130\n",
      "2503/3150: train loss: 0.015626\n",
      "2504/3150: train loss: 0.025232\n",
      "2505/3150: train loss: 0.010047\n",
      "2506/3150: train loss: 0.051991\n",
      "2507/3150: train loss: 0.011043\n",
      "2508/3150: train loss: 0.015615\n",
      "2509/3150: train loss: 0.025921\n",
      "2510/3150: train loss: 0.024108\n",
      "2511/3150: train loss: 0.017823\n",
      "2512/3150: train loss: 0.027392\n",
      "2513/3150: train loss: 0.031738\n",
      "2514/3150: train loss: 0.019940\n",
      "2515/3150: train loss: 0.050203\n",
      "2516/3150: train loss: 0.017612\n",
      "2517/3150: train loss: 0.036052\n",
      "2518/3150: train loss: 0.035783\n",
      "2519/3150: train loss: 0.031546\n",
      "(0.9853333333333333, 0.9781904761904762)\n",
      "2520/3150: train loss: 0.017659\n",
      "2521/3150: train loss: 0.068421\n",
      "2522/3150: train loss: 0.030098\n",
      "2523/3150: train loss: 0.022400\n",
      "2524/3150: train loss: 0.025120\n",
      "2525/3150: train loss: 0.041857\n",
      "2526/3150: train loss: 0.088622\n",
      "2527/3150: train loss: 0.032349\n",
      "2528/3150: train loss: 0.026122\n",
      "2529/3150: train loss: 0.025114\n",
      "2530/3150: train loss: 0.021944\n",
      "2531/3150: train loss: 0.024871\n",
      "2532/3150: train loss: 0.031514\n",
      "2533/3150: train loss: 0.017648\n",
      "2534/3150: train loss: 0.025823\n",
      "2535/3150: train loss: 0.021143\n",
      "2536/3150: train loss: 0.026824\n",
      "2537/3150: train loss: 0.015321\n",
      "2538/3150: train loss: 0.012271\n",
      "2539/3150: train loss: 0.021885\n",
      "2540/3150: train loss: 0.017761\n",
      "2541/3150: train loss: 0.055164\n",
      "2542/3150: train loss: 0.010835\n",
      "2543/3150: train loss: 0.024473\n",
      "2544/3150: train loss: 0.025801\n",
      "2545/3150: train loss: 0.024259\n",
      "2546/3150: train loss: 0.057853\n",
      "2547/3150: train loss: 0.038356\n",
      "2548/3150: train loss: 0.020297\n",
      "2549/3150: train loss: 0.035953\n",
      "2550/3150: train loss: 0.015802\n",
      "2551/3150: train loss: 0.015856\n",
      "2552/3150: train loss: 0.016458\n",
      "2553/3150: train loss: 0.036091\n",
      "2554/3150: train loss: 0.047263\n",
      "2555/3150: train loss: 0.016615\n",
      "2556/3150: train loss: 0.057255\n",
      "2557/3150: train loss: 0.023220\n",
      "2558/3150: train loss: 0.027009\n",
      "2559/3150: train loss: 0.048290\n",
      "2560/3150: train loss: 0.017599\n",
      "2561/3150: train loss: 0.021062\n",
      "2562/3150: train loss: 0.028053\n",
      "2563/3150: train loss: 0.036125\n",
      "2564/3150: train loss: 0.031378\n",
      "2565/3150: train loss: 0.029508\n",
      "2566/3150: train loss: 0.057476\n",
      "2567/3150: train loss: 0.018861\n",
      "2568/3150: train loss: 0.011636\n",
      "2569/3150: train loss: 0.021763\n",
      "2570/3150: train loss: 0.031140\n",
      "2571/3150: train loss: 0.052401\n",
      "2572/3150: train loss: 0.019159\n",
      "2573/3150: train loss: 0.051650\n",
      "2574/3150: train loss: 0.021122\n",
      "2575/3150: train loss: 0.009313\n",
      "2576/3150: train loss: 0.027027\n",
      "2577/3150: train loss: 0.014850\n",
      "2578/3150: train loss: 0.011268\n",
      "2579/3150: train loss: 0.034597\n",
      "2580/3150: train loss: 0.014591\n",
      "2581/3150: train loss: 0.014282\n",
      "2582/3150: train loss: 0.019151\n",
      "2583/3150: train loss: 0.024784\n",
      "2584/3150: train loss: 0.009762\n",
      "2585/3150: train loss: 0.029664\n",
      "2586/3150: train loss: 0.026512\n",
      "2587/3150: train loss: 0.009006\n",
      "2588/3150: train loss: 0.030487\n",
      "2589/3150: train loss: 0.016154\n",
      "2590/3150: train loss: 0.083141\n",
      "2591/3150: train loss: 0.019729\n",
      "2592/3150: train loss: 0.029499\n",
      "2593/3150: train loss: 0.045026\n",
      "2594/3150: train loss: 0.035467\n",
      "2595/3150: train loss: 0.026337\n",
      "2596/3150: train loss: 0.015912\n",
      "2597/3150: train loss: 0.026943\n",
      "2598/3150: train loss: 0.037727\n",
      "2599/3150: train loss: 0.031442\n",
      "2600/3150: train loss: 0.015588\n",
      "2601/3150: train loss: 0.013869\n",
      "2602/3150: train loss: 0.037466\n",
      "2603/3150: train loss: 0.021532\n",
      "2604/3150: train loss: 0.017228\n",
      "2605/3150: train loss: 0.011006\n",
      "2606/3150: train loss: 0.098046\n",
      "2607/3150: train loss: 0.010087\n",
      "2608/3150: train loss: 0.056138\n",
      "2609/3150: train loss: 0.012896\n",
      "2610/3150: train loss: 0.038085\n",
      "2611/3150: train loss: 0.014941\n",
      "2612/3150: train loss: 0.047536\n",
      "2613/3150: train loss: 0.043510\n",
      "2614/3150: train loss: 0.018261\n",
      "2615/3150: train loss: 0.018749\n",
      "2616/3150: train loss: 0.029473\n",
      "2617/3150: train loss: 0.017404\n",
      "2618/3150: train loss: 0.028302\n",
      "2619/3150: train loss: 0.062422\n",
      "2620/3150: train loss: 0.024279\n",
      "2621/3150: train loss: 0.022141\n",
      "2622/3150: train loss: 0.010379\n",
      "2623/3150: train loss: 0.035948\n",
      "2624/3150: train loss: 0.034650\n",
      "2625/3150: train loss: 0.014035\n",
      "2626/3150: train loss: 0.035153\n",
      "2627/3150: train loss: 0.014481\n",
      "2628/3150: train loss: 0.037000\n",
      "2629/3150: train loss: 0.036259\n",
      "2630/3150: train loss: 0.026098\n",
      "2631/3150: train loss: 0.074330\n",
      "2632/3150: train loss: 0.029466\n",
      "2633/3150: train loss: 0.015945\n",
      "2634/3150: train loss: 0.026188\n",
      "2635/3150: train loss: 0.027668\n",
      "2636/3150: train loss: 0.020145\n",
      "2637/3150: train loss: 0.013643\n",
      "2638/3150: train loss: 0.017415\n",
      "2639/3150: train loss: 0.031441\n",
      "2640/3150: train loss: 0.032782\n",
      "2641/3150: train loss: 0.028057\n",
      "2642/3150: train loss: 0.050586\n",
      "2643/3150: train loss: 0.037976\n",
      "2644/3150: train loss: 0.047818\n",
      "2645/3150: train loss: 0.065454\n",
      "2646/3150: train loss: 0.034286\n",
      "2647/3150: train loss: 0.055908\n",
      "2648/3150: train loss: 0.023890\n",
      "2649/3150: train loss: 0.012474\n",
      "2650/3150: train loss: 0.013638\n",
      "2651/3150: train loss: 0.023545\n",
      "2652/3150: train loss: 0.020738\n",
      "2653/3150: train loss: 0.020522\n",
      "2654/3150: train loss: 0.021593\n",
      "2655/3150: train loss: 0.016195\n",
      "2656/3150: train loss: 0.067313\n",
      "2657/3150: train loss: 0.034963\n",
      "2658/3150: train loss: 0.026069\n",
      "2659/3150: train loss: 0.036747\n",
      "2660/3150: train loss: 0.091561\n",
      "2661/3150: train loss: 0.008647\n",
      "2662/3150: train loss: 0.010998\n",
      "2663/3150: train loss: 0.027445\n",
      "2664/3150: train loss: 0.025598\n",
      "2665/3150: train loss: 0.016964\n",
      "2666/3150: train loss: 0.026272\n",
      "2667/3150: train loss: 0.108230\n",
      "2668/3150: train loss: 0.018936\n",
      "2669/3150: train loss: 0.039530\n",
      "2670/3150: train loss: 0.022865\n",
      "2671/3150: train loss: 0.064975\n",
      "2672/3150: train loss: 0.017505\n",
      "2673/3150: train loss: 0.019861\n",
      "2674/3150: train loss: 0.021449\n",
      "2675/3150: train loss: 0.024916\n",
      "2676/3150: train loss: 0.013655\n",
      "2677/3150: train loss: 0.016242\n",
      "2678/3150: train loss: 0.067004\n",
      "2679/3150: train loss: 0.024522\n",
      "2680/3150: train loss: 0.029418\n",
      "2681/3150: train loss: 0.008311\n",
      "2682/3150: train loss: 0.036025\n",
      "2683/3150: train loss: 0.041919\n",
      "2684/3150: train loss: 0.021384\n",
      "2685/3150: train loss: 0.020830\n",
      "2686/3150: train loss: 0.029901\n",
      "2687/3150: train loss: 0.024930\n",
      "2688/3150: train loss: 0.038871\n",
      "2689/3150: train loss: 0.021657\n",
      "2690/3150: train loss: 0.014150\n",
      "2691/3150: train loss: 0.041411\n",
      "2692/3150: train loss: 0.025684\n",
      "2693/3150: train loss: 0.047527\n",
      "2694/3150: train loss: 0.051302\n",
      "2695/3150: train loss: 0.015574\n",
      "2696/3150: train loss: 0.022444\n",
      "2697/3150: train loss: 0.053711\n",
      "2698/3150: train loss: 0.017069\n",
      "2699/3150: train loss: 0.103314\n",
      "2700/3150: train loss: 0.026450\n",
      "2701/3150: train loss: 0.030731\n",
      "2702/3150: train loss: 0.030469\n",
      "2703/3150: train loss: 0.016393\n",
      "2704/3150: train loss: 0.009511\n",
      "2705/3150: train loss: 0.021076\n",
      "2706/3150: train loss: 0.020235\n",
      "2707/3150: train loss: 0.016228\n",
      "2708/3150: train loss: 0.014981\n",
      "2709/3150: train loss: 0.012102\n",
      "2710/3150: train loss: 0.046050\n",
      "2711/3150: train loss: 0.029705\n",
      "2712/3150: train loss: 0.015809\n",
      "2713/3150: train loss: 0.010280\n",
      "2714/3150: train loss: 0.035108\n",
      "2715/3150: train loss: 0.018746\n",
      "2716/3150: train loss: 0.017147\n",
      "2717/3150: train loss: 0.055455\n",
      "2718/3150: train loss: 0.055594\n",
      "2719/3150: train loss: 0.021263\n",
      "2720/3150: train loss: 0.022909\n",
      "2721/3150: train loss: 0.045510\n",
      "2722/3150: train loss: 0.026735\n",
      "2723/3150: train loss: 0.024668\n",
      "2724/3150: train loss: 0.020976\n",
      "2725/3150: train loss: 0.019223\n",
      "2726/3150: train loss: 0.067796\n",
      "2727/3150: train loss: 0.023196\n",
      "2728/3150: train loss: 0.032366\n",
      "2729/3150: train loss: 0.059607\n",
      "2730/3150: train loss: 0.020781\n",
      "2731/3150: train loss: 0.025731\n",
      "2732/3150: train loss: 0.027165\n",
      "2733/3150: train loss: 0.084284\n",
      "2734/3150: train loss: 0.023563\n",
      "2735/3150: train loss: 0.085797\n",
      "2736/3150: train loss: 0.011982\n",
      "2737/3150: train loss: 0.030316\n",
      "2738/3150: train loss: 0.011427\n",
      "2739/3150: train loss: 0.034029\n",
      "2740/3150: train loss: 0.082313\n",
      "2741/3150: train loss: 0.015254\n",
      "2742/3150: train loss: 0.022995\n",
      "2743/3150: train loss: 0.018226\n",
      "2744/3150: train loss: 0.024185\n",
      "2745/3150: train loss: 0.014816\n",
      "2746/3150: train loss: 0.017229\n",
      "2747/3150: train loss: 0.028975\n",
      "2748/3150: train loss: 0.018044\n",
      "2749/3150: train loss: 0.013167\n",
      "2750/3150: train loss: 0.034939\n",
      "2751/3150: train loss: 0.026407\n",
      "2752/3150: train loss: 0.057956\n",
      "2753/3150: train loss: 0.045754\n",
      "2754/3150: train loss: 0.025415\n",
      "2755/3150: train loss: 0.030128\n",
      "2756/3150: train loss: 0.015229\n",
      "2757/3150: train loss: 0.017887\n",
      "2758/3150: train loss: 0.011641\n",
      "2759/3150: train loss: 0.032551\n",
      "2760/3150: train loss: 0.019696\n",
      "2761/3150: train loss: 0.032709\n",
      "2762/3150: train loss: 0.056173\n",
      "2763/3150: train loss: 0.022830\n",
      "2764/3150: train loss: 0.046097\n",
      "2765/3150: train loss: 0.018891\n",
      "2766/3150: train loss: 0.009007\n",
      "2767/3150: train loss: 0.020524\n",
      "2768/3150: train loss: 0.018438\n",
      "2769/3150: train loss: 0.022196\n",
      "2770/3150: train loss: 0.022947\n",
      "2771/3150: train loss: 0.020638\n",
      "2772/3150: train loss: 0.024861\n",
      "2773/3150: train loss: 0.024914\n",
      "2774/3150: train loss: 0.030875\n",
      "2775/3150: train loss: 0.022794\n",
      "2776/3150: train loss: 0.046555\n",
      "2777/3150: train loss: 0.054367\n",
      "2778/3150: train loss: 0.020374\n",
      "2779/3150: train loss: 0.029712\n",
      "2780/3150: train loss: 0.035830\n",
      "2781/3150: train loss: 0.012543\n",
      "2782/3150: train loss: 0.039659\n",
      "2783/3150: train loss: 0.008962\n",
      "2784/3150: train loss: 0.082595\n",
      "2785/3150: train loss: 0.020355\n",
      "2786/3150: train loss: 0.025697\n",
      "2787/3150: train loss: 0.017702\n",
      "2788/3150: train loss: 0.028849\n",
      "2789/3150: train loss: 0.023603\n",
      "2790/3150: train loss: 0.030720\n",
      "2791/3150: train loss: 0.026617\n",
      "2792/3150: train loss: 0.023497\n",
      "2793/3150: train loss: 0.038767\n",
      "2794/3150: train loss: 0.008575\n",
      "2795/3150: train loss: 0.019553\n",
      "2796/3150: train loss: 0.017663\n",
      "2797/3150: train loss: 0.024265\n",
      "2798/3150: train loss: 0.020065\n",
      "2799/3150: train loss: 0.024905\n",
      "2800/3150: train loss: 0.019127\n",
      "2801/3150: train loss: 0.019664\n",
      "2802/3150: train loss: 0.042262\n",
      "2803/3150: train loss: 0.020043\n",
      "2804/3150: train loss: 0.040380\n",
      "2805/3150: train loss: 0.018698\n",
      "2806/3150: train loss: 0.042332\n",
      "2807/3150: train loss: 0.057310\n",
      "2808/3150: train loss: 0.013169\n",
      "2809/3150: train loss: 0.054402\n",
      "2810/3150: train loss: 0.016296\n",
      "2811/3150: train loss: 0.026699\n",
      "2812/3150: train loss: 0.048382\n",
      "2813/3150: train loss: 0.030420\n",
      "2814/3150: train loss: 0.023985\n",
      "2815/3150: train loss: 0.021684\n",
      "2816/3150: train loss: 0.012083\n",
      "2817/3150: train loss: 0.037182\n",
      "2818/3150: train loss: 0.013075\n",
      "2819/3150: train loss: 0.016140\n",
      "2820/3150: train loss: 0.029185\n",
      "2821/3150: train loss: 0.015594\n",
      "2822/3150: train loss: 0.017592\n",
      "2823/3150: train loss: 0.032078\n",
      "2824/3150: train loss: 0.035470\n",
      "2825/3150: train loss: 0.020306\n",
      "2826/3150: train loss: 0.013444\n",
      "2827/3150: train loss: 0.027244\n",
      "2828/3150: train loss: 0.017025\n",
      "2829/3150: train loss: 0.014085\n",
      "2830/3150: train loss: 0.017961\n",
      "2831/3150: train loss: 0.026098\n",
      "2832/3150: train loss: 0.009494\n",
      "2833/3150: train loss: 0.023225\n",
      "2834/3150: train loss: 0.051906\n",
      "(0.9862222222222222, 0.9797142857142858)\n",
      "2835/3150: train loss: 0.022569\n",
      "2836/3150: train loss: 0.017491\n",
      "2837/3150: train loss: 0.019245\n",
      "2838/3150: train loss: 0.023486\n",
      "2839/3150: train loss: 0.028022\n",
      "2840/3150: train loss: 0.061376\n",
      "2841/3150: train loss: 0.045390\n",
      "2842/3150: train loss: 0.012618\n",
      "2843/3150: train loss: 0.018299\n",
      "2844/3150: train loss: 0.010482\n",
      "2845/3150: train loss: 0.063796\n",
      "2846/3150: train loss: 0.029529\n",
      "2847/3150: train loss: 0.013002\n",
      "2848/3150: train loss: 0.008071\n",
      "2849/3150: train loss: 0.020931\n",
      "2850/3150: train loss: 0.016827\n",
      "2851/3150: train loss: 0.017493\n",
      "2852/3150: train loss: 0.021676\n",
      "2853/3150: train loss: 0.027230\n",
      "2854/3150: train loss: 0.017629\n",
      "2855/3150: train loss: 0.020491\n",
      "2856/3150: train loss: 0.007457\n",
      "2857/3150: train loss: 0.014366\n",
      "2858/3150: train loss: 0.090793\n",
      "2859/3150: train loss: 0.037455\n",
      "2860/3150: train loss: 0.018500\n",
      "2861/3150: train loss: 0.029144\n",
      "2862/3150: train loss: 0.049009\n",
      "2863/3150: train loss: 0.042554\n",
      "2864/3150: train loss: 0.014383\n",
      "2865/3150: train loss: 0.067574\n",
      "2866/3150: train loss: 0.083053\n",
      "2867/3150: train loss: 0.069851\n",
      "2868/3150: train loss: 0.036319\n",
      "2869/3150: train loss: 0.031375\n",
      "2870/3150: train loss: 0.010427\n",
      "2871/3150: train loss: 0.089738\n",
      "2872/3150: train loss: 0.027407\n",
      "2873/3150: train loss: 0.013435\n",
      "2874/3150: train loss: 0.024529\n",
      "2875/3150: train loss: 0.056108\n",
      "2876/3150: train loss: 0.016975\n",
      "2877/3150: train loss: 0.015883\n",
      "2878/3150: train loss: 0.012193\n",
      "2879/3150: train loss: 0.021148\n",
      "2880/3150: train loss: 0.036935\n",
      "2881/3150: train loss: 0.018976\n",
      "2882/3150: train loss: 0.024194\n",
      "2883/3150: train loss: 0.054254\n",
      "2884/3150: train loss: 0.006797\n",
      "2885/3150: train loss: 0.023831\n",
      "2886/3150: train loss: 0.026750\n",
      "2887/3150: train loss: 0.015980\n",
      "2888/3150: train loss: 0.035448\n",
      "2889/3150: train loss: 0.031618\n",
      "2890/3150: train loss: 0.024945\n",
      "2891/3150: train loss: 0.017109\n",
      "2892/3150: train loss: 0.012167\n",
      "2893/3150: train loss: 0.021718\n",
      "2894/3150: train loss: 0.064396\n",
      "2895/3150: train loss: 0.031090\n",
      "2896/3150: train loss: 0.043167\n",
      "2897/3150: train loss: 0.022373\n",
      "2898/3150: train loss: 0.034297\n",
      "2899/3150: train loss: 0.034432\n",
      "2900/3150: train loss: 0.018669\n",
      "2901/3150: train loss: 0.040983\n",
      "2902/3150: train loss: 0.014051\n",
      "2903/3150: train loss: 0.009481\n",
      "2904/3150: train loss: 0.024506\n",
      "2905/3150: train loss: 0.041328\n",
      "2906/3150: train loss: 0.021637\n",
      "2907/3150: train loss: 0.031894\n",
      "2908/3150: train loss: 0.041561\n",
      "2909/3150: train loss: 0.018824\n",
      "2910/3150: train loss: 0.005916\n",
      "2911/3150: train loss: 0.014262\n",
      "2912/3150: train loss: 0.007442\n",
      "2913/3150: train loss: 0.027507\n",
      "2914/3150: train loss: 0.026877\n",
      "2915/3150: train loss: 0.050245\n",
      "2916/3150: train loss: 0.024777\n",
      "2917/3150: train loss: 0.017924\n",
      "2918/3150: train loss: 0.018898\n",
      "2919/3150: train loss: 0.036788\n",
      "2920/3150: train loss: 0.029431\n",
      "2921/3150: train loss: 0.025494\n",
      "2922/3150: train loss: 0.026460\n",
      "2923/3150: train loss: 0.024472\n",
      "2924/3150: train loss: 0.023241\n",
      "2925/3150: train loss: 0.029940\n",
      "2926/3150: train loss: 0.052759\n",
      "2927/3150: train loss: 0.012242\n",
      "2928/3150: train loss: 0.027997\n",
      "2929/3150: train loss: 0.025367\n",
      "2930/3150: train loss: 0.016863\n",
      "2931/3150: train loss: 0.026308\n",
      "2932/3150: train loss: 0.010661\n",
      "2933/3150: train loss: 0.071265\n",
      "2934/3150: train loss: 0.039855\n",
      "2935/3150: train loss: 0.014318\n",
      "2936/3150: train loss: 0.020452\n",
      "2937/3150: train loss: 0.033411\n",
      "2938/3150: train loss: 0.023387\n",
      "2939/3150: train loss: 0.045931\n",
      "2940/3150: train loss: 0.025347\n",
      "2941/3150: train loss: 0.024117\n",
      "2942/3150: train loss: 0.019631\n",
      "2943/3150: train loss: 0.012233\n",
      "2944/3150: train loss: 0.008203\n",
      "2945/3150: train loss: 0.015098\n",
      "2946/3150: train loss: 0.018640\n",
      "2947/3150: train loss: 0.022772\n",
      "2948/3150: train loss: 0.012284\n",
      "2949/3150: train loss: 0.055210\n",
      "2950/3150: train loss: 0.044997\n",
      "2951/3150: train loss: 0.039655\n",
      "2952/3150: train loss: 0.039274\n",
      "2953/3150: train loss: 0.017574\n",
      "2954/3150: train loss: 0.027941\n",
      "2955/3150: train loss: 0.031868\n",
      "2956/3150: train loss: 0.033779\n",
      "2957/3150: train loss: 0.017947\n",
      "2958/3150: train loss: 0.048664\n",
      "2959/3150: train loss: 0.020937\n",
      "2960/3150: train loss: 0.014017\n",
      "2961/3150: train loss: 0.028971\n",
      "2962/3150: train loss: 0.015148\n",
      "2963/3150: train loss: 0.057132\n",
      "2964/3150: train loss: 0.014253\n",
      "2965/3150: train loss: 0.032254\n",
      "2966/3150: train loss: 0.014655\n",
      "2967/3150: train loss: 0.024282\n",
      "2968/3150: train loss: 0.048115\n",
      "2969/3150: train loss: 0.018543\n",
      "2970/3150: train loss: 0.016259\n",
      "2971/3150: train loss: 0.026192\n",
      "2972/3150: train loss: 0.022496\n",
      "2973/3150: train loss: 0.016796\n",
      "2974/3150: train loss: 0.022187\n",
      "2975/3150: train loss: 0.023257\n",
      "2976/3150: train loss: 0.028882\n",
      "2977/3150: train loss: 0.033293\n",
      "2978/3150: train loss: 0.032700\n",
      "2979/3150: train loss: 0.023745\n",
      "2980/3150: train loss: 0.009547\n",
      "2981/3150: train loss: 0.031146\n",
      "2982/3150: train loss: 0.016255\n",
      "2983/3150: train loss: 0.020956\n",
      "2984/3150: train loss: 0.024498\n",
      "2985/3150: train loss: 0.018734\n",
      "2986/3150: train loss: 0.025798\n",
      "2987/3150: train loss: 0.027799\n",
      "2988/3150: train loss: 0.006901\n",
      "2989/3150: train loss: 0.009342\n",
      "2990/3150: train loss: 0.050937\n",
      "2991/3150: train loss: 0.025782\n",
      "2992/3150: train loss: 0.046902\n",
      "2993/3150: train loss: 0.014045\n",
      "2994/3150: train loss: 0.016873\n",
      "2995/3150: train loss: 0.016197\n",
      "2996/3150: train loss: 0.015193\n",
      "2997/3150: train loss: 0.021967\n",
      "2998/3150: train loss: 0.027087\n",
      "2999/3150: train loss: 0.018953\n",
      "3000/3150: train loss: 0.089995\n",
      "3001/3150: train loss: 0.022235\n",
      "3002/3150: train loss: 0.021871\n",
      "3003/3150: train loss: 0.033218\n",
      "3004/3150: train loss: 0.027863\n",
      "3005/3150: train loss: 0.025122\n",
      "3006/3150: train loss: 0.016670\n",
      "3007/3150: train loss: 0.021072\n",
      "3008/3150: train loss: 0.020087\n",
      "3009/3150: train loss: 0.048601\n",
      "3010/3150: train loss: 0.022087\n",
      "3011/3150: train loss: 0.028710\n",
      "3012/3150: train loss: 0.042216\n",
      "3013/3150: train loss: 0.086836\n",
      "3014/3150: train loss: 0.020680\n",
      "3015/3150: train loss: 0.005675\n",
      "3016/3150: train loss: 0.052161\n",
      "3017/3150: train loss: 0.010579\n",
      "3018/3150: train loss: 0.011055\n",
      "3019/3150: train loss: 0.038825\n",
      "3020/3150: train loss: 0.076138\n",
      "3021/3150: train loss: 0.019838\n",
      "3022/3150: train loss: 0.021066\n",
      "3023/3150: train loss: 0.014844\n",
      "3024/3150: train loss: 0.019335\n",
      "3025/3150: train loss: 0.050980\n",
      "3026/3150: train loss: 0.115081\n",
      "3027/3150: train loss: 0.032258\n",
      "3028/3150: train loss: 0.031887\n",
      "3029/3150: train loss: 0.008747\n",
      "3030/3150: train loss: 0.027609\n",
      "3031/3150: train loss: 0.040007\n",
      "3032/3150: train loss: 0.030396\n",
      "3033/3150: train loss: 0.017143\n",
      "3034/3150: train loss: 0.036536\n",
      "3035/3150: train loss: 0.013673\n",
      "3036/3150: train loss: 0.058018\n",
      "3037/3150: train loss: 0.067587\n",
      "3038/3150: train loss: 0.023341\n",
      "3039/3150: train loss: 0.050492\n",
      "3040/3150: train loss: 0.023140\n",
      "3041/3150: train loss: 0.044974\n",
      "3042/3150: train loss: 0.015718\n",
      "3043/3150: train loss: 0.088662\n",
      "3044/3150: train loss: 0.027049\n",
      "3045/3150: train loss: 0.043524\n",
      "3046/3150: train loss: 0.050459\n",
      "3047/3150: train loss: 0.016222\n",
      "3048/3150: train loss: 0.087200\n",
      "3049/3150: train loss: 0.016131\n",
      "3050/3150: train loss: 0.034221\n",
      "3051/3150: train loss: 0.016253\n",
      "3052/3150: train loss: 0.015908\n",
      "3053/3150: train loss: 0.017776\n",
      "3054/3150: train loss: 0.044619\n",
      "3055/3150: train loss: 0.031252\n",
      "3056/3150: train loss: 0.018799\n",
      "3057/3150: train loss: 0.010332\n",
      "3058/3150: train loss: 0.032701\n",
      "3059/3150: train loss: 0.021078\n",
      "3060/3150: train loss: 0.091119\n",
      "3061/3150: train loss: 0.026827\n",
      "3062/3150: train loss: 0.013498\n",
      "3063/3150: train loss: 0.019312\n",
      "3064/3150: train loss: 0.022331\n",
      "3065/3150: train loss: 0.074756\n",
      "3066/3150: train loss: 0.037580\n",
      "3067/3150: train loss: 0.033939\n",
      "3068/3150: train loss: 0.014335\n",
      "3069/3150: train loss: 0.013541\n",
      "3070/3150: train loss: 0.028528\n",
      "3071/3150: train loss: 0.017243\n",
      "3072/3150: train loss: 0.015621\n",
      "3073/3150: train loss: 0.014871\n",
      "3074/3150: train loss: 0.016347\n",
      "3075/3150: train loss: 0.019975\n",
      "3076/3150: train loss: 0.037036\n",
      "3077/3150: train loss: 0.034072\n",
      "3078/3150: train loss: 0.010409\n",
      "3079/3150: train loss: 0.021869\n",
      "3080/3150: train loss: 0.044981\n",
      "3081/3150: train loss: 0.062283\n",
      "3082/3150: train loss: 0.013448\n",
      "3083/3150: train loss: 0.033848\n",
      "3084/3150: train loss: 0.009182\n",
      "3085/3150: train loss: 0.018218\n",
      "3086/3150: train loss: 0.031517\n",
      "3087/3150: train loss: 0.012106\n",
      "3088/3150: train loss: 0.027421\n",
      "3089/3150: train loss: 0.011695\n",
      "3090/3150: train loss: 0.037709\n",
      "3091/3150: train loss: 0.026005\n",
      "3092/3150: train loss: 0.024076\n",
      "3093/3150: train loss: 0.028127\n",
      "3094/3150: train loss: 0.055871\n",
      "3095/3150: train loss: 0.013502\n",
      "3096/3150: train loss: 0.034406\n",
      "3097/3150: train loss: 0.019343\n",
      "3098/3150: train loss: 0.021745\n",
      "3099/3150: train loss: 0.042573\n",
      "3100/3150: train loss: 0.021889\n",
      "3101/3150: train loss: 0.016618\n",
      "3102/3150: train loss: 0.014075\n",
      "3103/3150: train loss: 0.016816\n",
      "3104/3150: train loss: 0.041613\n",
      "3105/3150: train loss: 0.021826\n",
      "3106/3150: train loss: 0.010306\n",
      "3107/3150: train loss: 0.016079\n",
      "3108/3150: train loss: 0.012446\n",
      "3109/3150: train loss: 0.012586\n",
      "3110/3150: train loss: 0.027804\n",
      "3111/3150: train loss: 0.005739\n",
      "3112/3150: train loss: 0.029120\n",
      "3113/3150: train loss: 0.035362\n",
      "3114/3150: train loss: 0.045291\n",
      "3115/3150: train loss: 0.012797\n",
      "3116/3150: train loss: 0.024309\n",
      "3117/3150: train loss: 0.019266\n",
      "3118/3150: train loss: 0.043469\n",
      "3119/3150: train loss: 0.029922\n",
      "3120/3150: train loss: 0.021807\n",
      "3121/3150: train loss: 0.025582\n",
      "3122/3150: train loss: 0.017229\n",
      "3123/3150: train loss: 0.033676\n",
      "3124/3150: train loss: 0.035423\n",
      "3125/3150: train loss: 0.040504\n",
      "3126/3150: train loss: 0.027113\n",
      "3127/3150: train loss: 0.013959\n",
      "3128/3150: train loss: 0.029375\n",
      "3129/3150: train loss: 0.014785\n",
      "3130/3150: train loss: 0.018510\n",
      "3131/3150: train loss: 0.018942\n",
      "3132/3150: train loss: 0.007613\n",
      "3133/3150: train loss: 0.039268\n",
      "3134/3150: train loss: 0.014526\n",
      "3135/3150: train loss: 0.033949\n",
      "3136/3150: train loss: 0.017901\n",
      "3137/3150: train loss: 0.013975\n",
      "3138/3150: train loss: 0.032586\n",
      "3139/3150: train loss: 0.023926\n",
      "3140/3150: train loss: 0.022553\n",
      "3141/3150: train loss: 0.013980\n",
      "3142/3150: train loss: 0.022822\n",
      "3143/3150: train loss: 0.019763\n",
      "3144/3150: train loss: 0.033908\n",
      "3145/3150: train loss: 0.055654\n",
      "3146/3150: train loss: 0.022473\n",
      "3147/3150: train loss: 0.073511\n",
      "3148/3150: train loss: 0.015440\n",
      "3149/3150: train loss: 0.027293\n",
      "(0.9870476190476191, 0.98)\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "iters_num = x_train.shape[0] / batch_size * epochs\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "h = None\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size/batch_size,1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size,batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = network.gradient(x_batch,t_batch)\n",
    "    \n",
    "    if h is None:\n",
    "        h = {}\n",
    "        for key , val in grad.items():\n",
    "            h[key] = np.zeros_like(val)\n",
    "    \n",
    "    for key in [\"W1\",\"b1\",\"W2\",\"b2\",\"W3\",\"b3\"]:\n",
    "        h[key] += grad[key] * grad[key]\n",
    "        network.params[key] -= learning_rate * grad[key] / np.sqrt(h[key]+1e-3)\n",
    "    \n",
    "    loss = network.loss(x_batch,t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    print(\"%d/%d: train loss: %lf\" % (i,iters_num,loss))\n",
    "    \n",
    "    if i % iter_per_epoch == iter_per_epoch - 1:\n",
    "        train_acc = network.accuracy(x_train,t_train)\n",
    "        test_acc = network.accuracy(x_test,t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        \n",
    "        print(train_acc,test_acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3SP7YPDHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1545f230>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAe7/A2QCVcvz8XNKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14165690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_list,label=\"loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_acc_list,label=\"train accuracy\")\n",
    "plt.plot(test_acc_list,label=\"test accuracy\")\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
