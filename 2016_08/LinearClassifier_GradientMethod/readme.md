線形分類器のお勉強  


オンライン機械学習(講談社)  
P30 アルゴリズム3.2  
勾配降下法による線形分類器の学習  
以下，実装に必要な数式の導出  


データが $t=1,...,N$  $( {\bf x}^{t} , y^t)$  みたいな形で与えられているものとする  
※ ${\bf x}$ が特徴ベクトル，$y$ がラベル(-1,1)

目的関数は
$$
L({\bf w}) = \sum_{i=1}^{N} {l({ \bf x}^{i} , y^i,{ \bf w})}
$$
ただし関数 $l({ \bf x}^{i} , y^i,{ \bf w})$ は正しく分類されていれば0，そうでなければその特徴ベクトルとの内積の絶対値を返す．

$$
l({ \bf x }^{i} , y^i,{ \bf w }) = max(-y^i { \bf w }^{T}  {\bf x}^i , 0)
$$

勾配降下法では重みを次のように変化させる．

$$
{\bf w}^{(t+1)} = { \bf w}^{(t)} + \eta ^{(t)} \nabla L({\bf w}^{(t)})
$$

これを実装するにあたって $\nabla L({\bf w}^{(t)})$ を考える必要がある．ここで目的関数 $L$ は

$$
L({\bf w}) = \sum_{i=1}^{N} {l({ \bf x}^{i} , y^i,{ \bf w})}
$$

というように関数 $l$ の線形和であるから

$$
\nabla L({\bf w}) = \sum_{i=1}^{N} {\nabla l({ \bf x}^{i} , y^i,{ \bf w})}
$$

と書き直すことが出来る．しかし関数 $l$ は分離超平面で関数の形が変わる……少し面倒なので数値的に微分することにします  

ちなみに勾配(gradient)を数値的に計算してくれる関数はscipy.optimizeのapprox_fprimeで実装されています

### 追記

目的関数について，
$$
l({ \bf x }^{i} , y^i,{ \bf w }) = max(-y^i { \bf w }^{T}  {\bf x}^i , 0)
$$
であるが，場合分けを利用して，
$$
  l({ \bf x }^{i} , y^i,{ \bf w }) = \begin{cases}
    0 & (-y^i { \bf w }^{T}  {\bf x}^i < 0 ) \\
    -y^i { \bf w }^{T}  {\bf x}^i & (otherwise)
  \end{cases}
$$
と書けることから，
$$
  \nabla l({ \bf x }^{i} , y^i,{ \bf w }) = \begin{cases}
    0 & (-y^i { \bf w }^{T}  {\bf x}^i < 0 ) \\
    -y^i  {\bf x}^i & (otherwise)
  \end{cases}
$$
と書くことで実装できる( 尚，　$-y^i { \bf w }^{T}  {\bf x}^i = 0$  のときは微分不可能なので，便宜上その微分を ${-y^i {\bf x}^i}$ として用いている．劣勾配，と言うらしい．)  
まぁとりあえず数値的に微分したやつでうまく行ったのでプログラムの変更はしない．
